{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PHW22/Assignment/blob/main/GenerativeAI/HW9_PHW%E3%80%8CQuick_Summary_of_Lecture_Video%E3%80%8D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MntmiANZCk2L"
      },
      "source": [
        "# GenAI HW9: Quick Summary of Lecture Video (演講影片快速摘要)\n",
        "## Objectives\n",
        "- ### Learn to quickly build applications related to speech recognition using existing APIs. (學習以現成的API快速搭建語音辨識相關的應用。)\n",
        "\n",
        "\n",
        "#### If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to ntu-gen-ai-2024-spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voEioD2DCoeq"
      },
      "source": [
        "# Part1 - Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSccLtt234Pm"
      },
      "source": [
        "## The lecture video provided for this assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgHVz9WF4Vfp"
      },
      "source": [
        "(1) For ease of processing, it has already been converted to a MP3 file.\n",
        "\n",
        "(2) If you would like to view the original video, the link is here:\n",
        "\n",
        "- 李琳山教授 信號與人生 (2023)\n",
        "\n",
        "  - https://www.youtube.com/watch?v=MxoQV4M0jY8\n",
        "\n",
        "\n",
        "(3) Since the original lecture video is quite long, we have edited the segment from 1:43:24 to 2:00:49 to use for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdoLJZE33oCD"
      },
      "source": [
        "## Install all necessary packages and import them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HREsIZV33yDy"
      },
      "source": [
        "The following code block takes about **150** seconds to run, but it may vary slightly depending on the condition of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MgL2wxdxCvA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1921391d-e5f9-4e3f-8916-bb9e199c3f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-4u16j3h1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-4u16j3h1\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=a19f79cb776f1e2dcc4658bc9e9176bef7a1a3a5cbf0aa1f770e834e4993d8c6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wlakloc_/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n",
            "Collecting srt\n",
            "  Downloading srt-3.5.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: srt\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.3-py3-none-any.whl size=22428 sha256=b91ea00c6a4afb2e64be48cba55aea9fba125b0bd2f3055ca8435ea0ea185d2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/31/a1/18e1e7e8bfdafd19e6803d7eb919b563dd11de380e4304e332\n",
            "Successfully built srt\n",
            "Installing collected packages: srt\n",
            "Successfully installed srt-3.5.3\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-6.4.post2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2023.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (67.7.2)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-5.5 zope.interface-6.4.post2\n",
            "Collecting opencc\n",
            "  Downloading OpenCC-1.1.7-cp310-cp310-manylinux1_x86_64.whl (779 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.8/779.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencc\n",
            "Successfully installed opencc-1.1.7\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.1 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.2 dill-0.3.8 multiprocess-0.70.16 requests-2.32.3 xxhash-3.4.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython) (67.7.2)\n",
            "Collecting jedi>=0.16 (from IPython)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython) (3.0.45)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n",
            "Collecting openai\n",
            "  Downloading openai-1.33.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.33.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anthropic\n",
            "  Downloading anthropic-0.28.0-py3-none-any.whl (862 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.7/862.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.27.0)\n",
            "Collecting jiter<1,>=0.4.0 (from anthropic)\n",
            "  Downloading jiter-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.18.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n",
            "Installing collected packages: jiter, anthropic\n",
            "Successfully installed anthropic-0.28.0 jiter-0.4.1\n"
          ]
        }
      ],
      "source": [
        "# Install packages.\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install srt\n",
        "!pip install datetime\n",
        "!pip install opencc\n",
        "!pip install datasets\n",
        "!pip install numpy\n",
        "!pip install soundfile\n",
        "!pip install IPython\n",
        "!pip install openai\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWqXz6C6omR9"
      },
      "source": [
        "The following code block takes about **5** seconds to run, but it may vary slightly depending on the condition of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JFwSa_x6C53S"
      },
      "outputs": [],
      "source": [
        "# Import packages.\n",
        "\n",
        "import whisper\n",
        "import srt\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import pathlib\n",
        "import textwrap\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from opencc import OpenCC\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from openai import OpenAI\n",
        "import google.generativeai as genai\n",
        "import anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFY6VDAyeooa"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBROu_HfgF1J"
      },
      "source": [
        "The code block below takes about **10** seconds to run, although there might be some slight variation depending on the state of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PAieqtY8evUJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "7c16f77332c4442f900d74757e09a4c3",
            "4436a5a5d544412187dd9e0bcc381dc3",
            "63014e48ddab48ae8364a5be1a5b35d4",
            "1da20e27b5204b1d8f73d2997bca7da8",
            "3d151547c8184fd79942d23ff6f07433",
            "a303666c673547b3bf806cbe69c23004",
            "d3ddc5c50e9f403eabaaa81f4a455228",
            "27f2d2759ba04a82b6869a4ab53b3d5a",
            "03c027ccf19042988e6e6bf5c338a331",
            "76d3e57c1cd54ab0bfe98ff2a8087b79",
            "959e8d6436af400fa49efcc16e699c32",
            "1a1c378508a74989a07cb7093e468541",
            "f507507f226c4061bf48a464305937a6",
            "36072d0d3103470e95ca68332df2d9ac",
            "1109f950409848638fb337ec956c7ddc",
            "5dc61916d95748e0b947fedc08d9e5a3",
            "38b1ce0d475046f4b11c596ceee21d12",
            "6fb8fbb7775940fcae0b3fef018fdfab",
            "fcaa00c15ade43518df22682e5c1285f",
            "b241cd8bfccf4f059fc6cba78385195f",
            "d9acf964f8884dbeb76ee96d224abe4e",
            "1b0dbe0d59d5477c8ea376aa5eb15b99",
            "8bca4eaad9f84858ad34cda4c434d9b0",
            "58518e26240b42649b9fcb26e1d78cd1",
            "b4e613d14fca4d4395e3b3830213cf4b",
            "370772192f70424a9e699e26044c8d64",
            "8c7aad1c38844ac4b132f92c5b67a0c0",
            "3410dce4c170459884c3677b75a7d8ff",
            "0f17d82d06bb47caa9b1c9094cac405f",
            "d4718e80a37c4a62b99ef3d28f99f733",
            "3f30b4639aba4a098316880d7b21e276",
            "5f44598d1ddf445eb12815f2cdc440fa",
            "7618d752971440e980090269ac512bbb"
          ]
        },
        "outputId": "2f083dc8-bb01-4a17-f48d-e51a9cc38525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/305 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c16f77332c4442f900d74757e09a4c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/3.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a1c378508a74989a07cb7093e468541"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bca4eaad9f84858ad34cda4c434d9b0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load dataset.\n",
        "dataset_name = \"kuanhuggingface/NTU-GenAI-2024-HW9\"\n",
        "dataset = load_dataset(dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pN3dOGyrI-"
      },
      "source": [
        "The code block below takes about **15** seconds to run, although there might be some slight variation depending on the state of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E68E8Ej2isAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d8c389-b0c6-4b89-85e7-955f064cddc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now, we are going to transcribe the audio: 李琳山教授 信號與人生 (2023) (ntu-gen-ai-2024-hw9-16k.mp3).\n"
          ]
        }
      ],
      "source": [
        "# Prepare audio.\n",
        "input_audio = dataset[\"test\"][\"audio\"][0]\n",
        "input_audio_name = input_audio[\"path\"]\n",
        "input_audio_array = input_audio[\"array\"].astype(np.float32)\n",
        "sampling_rate = input_audio[\"sampling_rate\"]\n",
        "\n",
        "print(f\"Now, we are going to transcribe the audio: 李琳山教授 信號與人生 (2023) ({input_audio_name}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxTn1CfzDCXy"
      },
      "source": [
        "# Part2 - Automatic Speech Recognition (ASR)\n",
        "The function \"speech_recognition\" aims to convert audio to subtitle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OmWjjLUGC9z3"
      },
      "outputs": [],
      "source": [
        "def speech_recognition(model_name, input_audio, output_subtitle_path, decode_options, cache_dir=\"./\"):\n",
        "    '''\n",
        "        (1) Objective:\n",
        "            - This function aims to convert audio to subtitle.\n",
        "\n",
        "        (2) Arguments:\n",
        "\n",
        "            - model_name (str):\n",
        "                The name of the model. There are five model sizes, including tiny, base, small, medium, large-v3.\n",
        "                For example, you can use 'tiny', 'base', 'small', 'medium', 'large-v3' to specify the model name.\n",
        "                You can see 'https://github.com/openai/whisper' for more details.\n",
        "\n",
        "            - input_audio (Union[str, np.ndarray, torch.Tensor]):\n",
        "                The path to the audio file to open, or the audio waveform\n",
        "                - For example, if your input audio path is 'input.wav', you can use 'input.wav' to specify the input audio path.\n",
        "                - For example, if your input audio array is 'audio_array', you can use 'audio_array' to specify the input audio array.\n",
        "\n",
        "            - output_subtitle_path (str):\n",
        "                The path of the output subtitle file.\n",
        "                For example, if you want to save the subtitle file as 'output.srt', you can use 'output.srt' to specify the output subtitle path.\n",
        "\n",
        "            - decode_options (dict):\n",
        "                The options for decoding the audio file, including 'initial_prompt', 'prompt', 'prefix', 'temperature'.\n",
        "                - initial_prompt (str):\n",
        "                    Optional text to provide as a prompt for the first window. This can be used to provide, or\n",
        "                    \"prompt-engineer\" a context for transcription, e.g. custom vocabularies or proper nouns\n",
        "                    to make it more likely to predict those word correctly.\n",
        "                    Default: None.\n",
        "\n",
        "                You can see \"https://github.com/openai/whisper/blob/main/whisper/decoding.py\" and \"https://github.com/openai/whisper/blob/main/whisper/transcribe.py\"\n",
        "                for more details.\n",
        "\n",
        "                - temperature (float):\n",
        "                    The temperature for sampling from the model. Higher values mean more randomness.\n",
        "                    Default: 0.0\n",
        "\n",
        "            - cache_dir (str):\n",
        "                The path of the cache directory for saving the model.\n",
        "                For example, if you want to save the cache files in 'cache' directory, you can use 'cache' to specify the cache directory.\n",
        "                Default: './'\n",
        "\n",
        "        (3) Example:\n",
        "\n",
        "            - If you want to use the 'base' model to convert 'input.wav' to 'output.srt' and save the cache files in 'cache' directory,\n",
        "            you can call this function as follows:\n",
        "\n",
        "                speech_recognition(model_name='base', input_audio_path='input.wav', output_subtitle_path='output.srt', cache_dir='cache')\n",
        "    '''\n",
        "\n",
        "    # Record the start time.\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"=============== Loading Whisper-{model_name} ===============\")\n",
        "\n",
        "    # Load the model.\n",
        "    model = whisper.load_model(name=model_name, download_root=cache_dir)\n",
        "\n",
        "    print(f\"Begin to utilize Whisper-{model_name} to transcribe the audio.\")\n",
        "\n",
        "    # Transcribe the audio.\n",
        "    transcription = model.transcribe(audio=input_audio, language=decode_options[\"language\"], verbose=False,\n",
        "                                     initial_prompt=decode_options[\"initial_prompt\"], temperature=decode_options[\"temperature\"])\n",
        "\n",
        "    # Record the end time.\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"The process of speech recognition costs {end_time - start_time} seconds.\")\n",
        "\n",
        "    subtitles = []\n",
        "    # Convert the transcription to subtitle and iterate over the segments.\n",
        "    for i, segment in tqdm(enumerate(transcription[\"segments\"])):\n",
        "\n",
        "        # Convert the start time to subtitle format.\n",
        "        start_time = datetime.timedelta(seconds=segment[\"start\"])\n",
        "\n",
        "        # Convert the end time to subtitle format.\n",
        "        end_time = datetime.timedelta(seconds=segment[\"end\"])\n",
        "\n",
        "        # Get the subtitle text.\n",
        "        text = segment[\"text\"]\n",
        "\n",
        "        # Append the subtitle to the subtitle list.\n",
        "        subtitles.append(srt.Subtitle(index=i, start=start_time, end=end_time, content=text))\n",
        "\n",
        "    # Convert the subtitle list to subtitle content.\n",
        "    srt_content = srt.compose(subtitles)\n",
        "\n",
        "    print(f\"\\n=============== Saving the subtitle to {output_subtitle_path} ===============\")\n",
        "\n",
        "    # Save the subtitle content to the subtitle file.\n",
        "    with open(output_subtitle_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(srt_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ZkyefXpvmh"
      },
      "source": [
        "In the following block, you can modify your desired parameters and the path of input file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UULEr1GpDAl6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of Whisper { run: \"auto\" }\n",
        "\n",
        "''' In this block, you can modify your desired parameters and the path of input file. '''\n",
        "\n",
        "# The name of the model you want to use.\n",
        "# For example, you can use 'tiny', 'base', 'small', 'medium', 'large-v3' to specify the model name.\n",
        "# @markdown **model_name**: The name of the model you want to use.\n",
        "model_name = \"medium\" # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v3\"]\n",
        "\n",
        "# Define the suffix of the output file.\n",
        "# @markdown **suffix**: The output file name is \"output-{suffix}.* \", where .* is the file extention (.txt or .srt)\n",
        "suffix = \"信號與人生\" # @param {type: \"string\"}\n",
        "\n",
        "# Path to the output file.\n",
        "output_subtitle_path = f\"./output-{suffix}.srt\"\n",
        "\n",
        "# Path of the output raw text file from the SRT file.\n",
        "output_raw_text_path = f\"./output-{suffix}.txt\"\n",
        "\n",
        "# Path to the directory where the model and dataset will be cached.\n",
        "cache_dir = \"./\"\n",
        "\n",
        "# The language of the lecture video.\n",
        "# @markdown **language**: The language of the lecture video.\n",
        "language = \"zh\" # @param {type:\"string\"}\n",
        "\n",
        "# Optional text to provide as a prompt for the first window.\n",
        "# @markdown **initial_prompt**: Optional text to provide as a prompt for the first window.\n",
        "initial_prompt = \"請用繁體中文\" #@param {type:\"string\"}\n",
        "\n",
        "# The temperature for sampling from the model. Higher values mean more randomness.\n",
        "# @markdown  **temperature**: The temperature for sampling from the model. Higher values mean more randomness.\n",
        "temperature = 0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TBhoPRKR9S4w"
      },
      "outputs": [],
      "source": [
        "# Construct DecodingOptions\n",
        "decode_options = {\n",
        "    \"language\": language,\n",
        "    \"initial_prompt\": initial_prompt,\n",
        "    \"temperature\": temperature\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4SfQ5Xn-fjya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e66bf04-53af-4aa1-ecf7-8b05d29f00fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting: (1) Model: whisper-medium (2) Language: zh (2) Initial Prompt: 請用繁體中文 (3) Temperature: 0\n",
            "Transcribe 李琳山教授 信號與人生 (2023)\n"
          ]
        }
      ],
      "source": [
        "# print message.\n",
        "message = \"Transcribe 李琳山教授 信號與人生 (2023)\"\n",
        "print(f\"Setting: (1) Model: whisper-{model_name} (2) Language: {language} (2) Initial Prompt: {initial_prompt} (3) Temperature: {temperature}\")\n",
        "print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxgZ2DNgpGlO"
      },
      "source": [
        "The code block below takes about **90 (240)** seconds to run when using the **base (medium)** model and **a T4 GPU**, although there might be some slight variation depending on the state of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SOULGnw5RF6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da47a557-fc3f-4285-e767-51145d1b8a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== Loading Whisper-medium ===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:12<00:00, 122MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin to utilize Whisper-medium to transcribe the audio.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 104500/104500 [02:39<00:00, 655.08frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The process of speech recognition costs 191.77863073349 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "370it [00:00, 161185.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=============== Saving the subtitle to ./output-信號與人生.srt ===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Running ASR.\n",
        "speech_recognition(model_name=model_name, input_audio=input_audio_array, output_subtitle_path=output_subtitle_path, decode_options=decode_options, cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgmFtnti1qhU"
      },
      "source": [
        "You can check the result of automatic speech recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XeU54f5X1erZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c5a071-d892-4229-9c8e-fda71e668e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "00:00:00,000 --> 00:00:04,000\n",
            "每次說這個學問是做出來的\n",
            "\n",
            "2\n",
            "00:00:06,000 --> 00:00:08,000\n",
            "什麼意思?\n",
            "\n",
            "3\n",
            "00:00:08,000 --> 00:00:12,000\n",
            "要做才會獲得學問\n",
            "\n",
            "4\n",
            "00:00:13,000 --> 00:00:16,000\n",
            "你如果每天光是坐在那裡聽\n",
            "\n",
            "5\n",
            "00:00:17,000 --> 00:00:20,000\n",
            "學問很可能是左耳進右耳出的\n",
            "\n",
            "6\n",
            "00:00:21,000 --> 00:00:23,000\n",
            "你光是坐在那兒讀\n",
            "\n",
            "7\n",
            "00:00:23,000 --> 00:00:26,000\n",
            "學問可能從眼睛進入腦海之後就忘掉了\n",
            "\n",
            "8\n",
            "00:00:26,000 --> 00:00:29,000\n",
            "如何能夠學問在腦海裡面\n",
            "\n",
            "9\n",
            "00:00:31,000 --> 00:00:33,000\n",
            "真的變成你自己學問\n",
            "\n",
            "10\n",
            "00:00:33,000 --> 00:00:35,000\n",
            "就是要做\n",
            "\n",
            "11\n",
            "00:00:36,000 --> 00:00:39,000\n",
            "可能有很多同學有這個經驗\n",
            "\n",
            "12\n",
            "00:00:39,000 --> 00:00:41,000\n",
            "你如果去修某一門課\n",
            "\n",
            "13\n",
            "00:00:41,000 --> 00:00:44,000\n",
            "或者做某一個實驗\n",
            "\n",
            "14\n",
            "00:00:44,000 --> 00:00:47,000\n",
            "在期末就是要教一個final project\n",
            "\n",
            "15\n",
            "00:00:48,000 --> 00:00:50,000\n",
            "那個final project就是要你把\n",
            "\n",
            "16\n",
            "00:00:51,000 --> 00:00:53,000\n",
            "學到的很多東西\n",
            "\n",
            "17\n",
            "00:00:53,000 --> 00:00:56,000\n",
            "最後整合在你的final project裡面\n",
            "\n",
            "18\n",
            "00:00:56,000 --> 00:00:58,000\n",
            "最後做出來的時候\n",
            "\n",
            "19\n",
            "00:00:58,000 --> 00:01:00,000\n",
            "就是把它們都整合了\n",
            "\n",
            "20\n",
            "00:01:00,000 --> 00:01:02,000\n",
            "當你學期結束\n",
            "\n",
            "21\n",
            "00:01:02,000 --> 00:01:04,000\n",
            "真的把final project做完的時候\n",
            "\n",
            "22\n",
            "00:01:04,000 --> 00:01:05,000\n",
            "你會忽然發現\n",
            "\n",
            "23\n",
            "00:01:05,000 --> 00:01:07,000\n",
            "我真的學到很多東西\n",
            "\n",
            "24\n",
            "00:01:07,000 --> 00:01:10,000\n",
            "那就是做出來的學問\n",
            "\n",
            "25\n",
            "00:01:10,000 --> 00:01:13,000\n",
            "也許可以舉另外一個例子\n",
            "\n",
            "26\n",
            "00:01:13,000 --> 00:01:16,000\n",
            "就是你如果學了某一些很複雜的演算法\n",
            "\n",
            "27\n",
            "00:01:16,000 --> 00:01:17,000\n",
            "或者什麼\n",
            "\n",
            "28\n",
            "00:01:17,000 --> 00:01:21,000\n",
            "好像覺得那些不見得在你的腦海裡\n",
            "\n",
            "29\n",
            "00:01:21,000 --> 00:01:24,000\n",
            "可是後來老師出了個習題\n",
            "\n",
            "30\n",
            "00:01:24,000 --> 00:01:26,000\n",
            "那個習題教你寫一個很大的程式\n",
            "\n",
            "31\n",
            "00:01:26,000 --> 00:01:28,000\n",
            "要把所有東西都包進去\n",
            "\n",
            "32\n",
            "00:01:28,000 --> 00:01:31,000\n",
            "當你把這個程式寫完的時候你會發現\n",
            "\n",
            "33\n",
            "00:01:31,000 --> 00:01:35,000\n",
            "你忽然把演算法裡所有東西都弄通了\n",
            "\n",
            "34\n",
            "00:01:35,000 --> 00:01:38,000\n",
            "那就是學問是做出來的\n",
            "\n",
            "35\n",
            "00:01:38,000 --> 00:01:40,000\n",
            "所以我們永遠要記得\n",
            "\n",
            "36\n",
            "00:01:40,000 --> 00:01:44,000\n",
            "盡量多動手多做\n",
            "\n",
            "37\n",
            "00:01:44,000 --> 00:01:46,000\n",
            "在動手跟做的過程之中\n",
            "\n",
            "38\n",
            "00:01:46,000 --> 00:01:49,000\n",
            "學問才可以變成是自己的\n",
            "\n",
            "39\n",
            "00:01:49,000 --> 00:01:51,000\n",
            "同樣的情形就是說\n",
            "\n",
            "40\n",
            "00:01:51,000 --> 00:01:57,000\n",
            "很多時候這樣動手或者做的表現或者成績\n",
            "\n",
            "41\n",
            "00:01:57,000 --> 00:02:00,000\n",
            "沒有一個成績單上的數字\n",
            "\n",
            "42\n",
            "00:02:00,000 --> 00:02:03,000\n",
            "使得很多人覺得那不重要\n",
            "\n",
            "43\n",
            "00:02:03,000 --> 00:02:07,000\n",
            "很多人甚至覺得這門課要做final project\n",
            "\n",
            "44\n",
            "00:02:07,000 --> 00:02:09,000\n",
            "我就不修了太累了\n",
            "\n",
            "45\n",
            "00:02:09,000 --> 00:02:12,000\n",
            "或者說那門課需要怎麼樣怎麼樣太累\n",
            "\n",
            "46\n",
            "00:02:12,000 --> 00:02:13,000\n",
            "我就不要做了\n",
            "\n",
            "47\n",
            "00:02:13,000 --> 00:02:17,000\n",
            "而不知道其實那個才是讓你做的機會\n",
            "\n",
            "48\n",
            "00:02:17,000 --> 00:02:19,000\n",
            "然後可以學到最多\n",
            "\n",
            "49\n",
            "00:02:19,000 --> 00:02:24,000\n",
            "也就是說雖然很可能那麼辛苦的做很多事\n",
            "\n",
            "50\n",
            "00:02:24,000 --> 00:02:27,000\n",
            "沒有讓你獲得什麼具體成績\n",
            "\n",
            "51\n",
            "00:02:27,000 --> 00:02:30,000\n",
            "對你的overfitting可能沒有幫助\n",
            "\n",
            "52\n",
            "00:02:30,000 --> 00:02:33,000\n",
            "可是對你的全面學習是很有幫助\n",
            "\n",
            "53\n",
            "00:02:33,000 --> 00:02:35,000\n",
            "是該學的\n",
            "\n",
            "54\n",
            "00:02:35,000 --> 00:02:38,000\n",
            "那不要漏掉這些事\n",
            "\n",
            "55\n",
            "00:02:38,000 --> 00:02:41,000\n",
            "那這是我所說的\n",
            "\n",
            "56\n",
            "00:02:41,000 --> 00:02:46,000\n",
            "那這個課業內可以做的這些事\n",
            "\n",
            "57\n",
            "00:02:46,000 --> 00:02:50,000\n",
            "那剛才我們講到思考的時候\n",
            "\n",
            "58\n",
            "00:02:50,000 --> 00:02:52,000\n",
            "我覺得我漏掉一點\n",
            "\n",
            "59\n",
            "00:02:52,000 --> 00:02:56,000\n",
            "你如果修我的信號課你可能會發現\n",
            "\n",
            "60\n",
            "00:02:56,000 --> 00:03:00,000\n",
            "我上課沒講到一個數學式子的時候\n",
            "\n",
            "61\n",
            "00:03:00,000 --> 00:03:02,000\n",
            "我通常都不推他的\n",
            "\n",
            "62\n",
            "00:03:02,000 --> 00:03:06,000\n",
            "我是在解釋那個數學式子在說什麼話\n",
            "\n",
            "63\n",
            "00:03:06,000 --> 00:03:08,000\n",
            "同樣的呢\n",
            "\n",
            "64\n",
            "00:03:08,000 --> 00:03:10,000\n",
            "沒講到一個什麼什麼事情的時候\n",
            "\n",
            "65\n",
            "00:03:10,000 --> 00:03:14,000\n",
            "我通常就在解釋他在說什麼話\n",
            "\n",
            "66\n",
            "00:03:14,000 --> 00:03:16,000\n",
            "也就是說\n",
            "\n",
            "67\n",
            "00:03:16,000 --> 00:03:20,000\n",
            "我在講的就是我讀到特本那裡的時候\n",
            "\n",
            "68\n",
            "00:03:20,000 --> 00:03:22,000\n",
            "我心裡怎麼想的\n",
            "\n",
            "69\n",
            "00:03:22,000 --> 00:03:28,000\n",
            "也就是我在告訴同學如何這個讀書的時候\n",
            "\n",
            "70\n",
            "00:03:28,000 --> 00:03:32,000\n",
            "如何一面讀一面練習思考\n",
            "\n",
            "71\n",
            "00:03:32,000 --> 00:03:37,000\n",
            "那這個才是最重要的一件事\n",
            "\n",
            "72\n",
            "00:03:37,000 --> 00:03:40,000\n",
            "如何培養自己思考的能力\n",
            "\n",
            "73\n",
            "00:03:40,000 --> 00:03:42,000\n",
            "跟培養思考的習慣\n",
            "\n",
            "74\n",
            "00:03:42,000 --> 00:03:46,000\n",
            "我覺得最好的辦法就是讀書的時候\n",
            "\n",
            "75\n",
            "00:03:46,000 --> 00:03:50,000\n",
            "凡是讀到一個數學式子都去想一想\n",
            "\n",
            "76\n",
            "00:03:50,000 --> 00:03:53,000\n",
            "那個數學式子到底在說什麼\n",
            "\n",
            "77\n",
            "00:03:53,000 --> 00:03:57,000\n",
            "凡是讀到特本上講什麼就去想一想\n",
            "\n",
            "78\n",
            "00:03:57,000 --> 00:03:59,000\n",
            "那個到底在說什麼\n",
            "\n",
            "79\n",
            "00:03:59,000 --> 00:04:03,000\n",
            "你要真的了解他在說什麼的時候\n",
            "\n",
            "80\n",
            "00:04:03,000 --> 00:04:06,000\n",
            "你就用了很多思考的功夫\n",
            "\n",
            "81\n",
            "00:04:06,000 --> 00:04:09,000\n",
            "你就在練習自己思考的能力了\n",
            "\n",
            "82\n",
            "00:04:09,000 --> 00:04:14,000\n",
            "好 以上說的是課業內的部分\n",
            "\n",
            "83\n",
            "00:04:14,000 --> 00:04:18,000\n",
            "那當然除了課業內之外呢\n",
            "\n",
            "84\n",
            "00:04:18,000 --> 00:04:21,000\n",
            "還有一大堆是不在課業內的\n",
            "\n",
            "85\n",
            "00:04:21,000 --> 00:04:24,000\n",
            "那就是課業外的\n",
            "\n",
            "86\n",
            "00:04:24,000 --> 00:04:27,000\n",
            "課業外也有很多式的\n",
            "\n",
            "87\n",
            "00:04:27,000 --> 00:04:33,000\n",
            "那我們可以舉例來說\n",
            "\n",
            "88\n",
            "00:04:33,000 --> 00:04:38,000\n",
            "課業外有什麼可以學習的\n",
            "\n",
            "89\n",
            "00:04:38,000 --> 00:04:42,000\n",
            "那我通常把學習定義成為\n",
            "\n",
            "90\n",
            "00:04:42,000 --> 00:04:43,000\n",
            "什麼是學習\n",
            "\n",
            "91\n",
            "00:04:43,000 --> 00:04:49,000\n",
            "學習就是一種增長\n",
            "\n",
            "92\n",
            "00:04:49,000 --> 00:04:53,000\n",
            "一種進步\n",
            "\n",
            "93\n",
            "00:04:53,000 --> 00:04:57,000\n",
            "然後獲得快樂\n",
            "\n",
            "94\n",
            "00:04:57,000 --> 00:05:00,000\n",
            "這就是學習\n",
            "\n",
            "95\n",
            "00:05:00,000 --> 00:05:03,000\n",
            "所以即使是課業外的任何事情\n",
            "\n",
            "96\n",
            "00:05:03,000 --> 00:05:05,000\n",
            "只要你覺得是有增長的\n",
            "\n",
            "97\n",
            "00:05:05,000 --> 00:05:07,000\n",
            "是有進步的\n",
            "\n",
            "98\n",
            "00:05:07,000 --> 00:05:08,000\n",
            "讓你覺得快樂的\n",
            "\n",
            "99\n",
            "00:05:08,000 --> 00:05:12,000\n",
            "那應該就是值得學習的地方\n",
            "\n",
            "100\n",
            "00:05:12,000 --> 00:05:16,000\n",
            "那我們可以舉很多例子\n",
            "\n",
            "101\n",
            "00:05:16,000 --> 00:05:20,000\n",
            "譬如說很多同學喜歡打球\n",
            "\n",
            "102\n",
            "00:05:20,000 --> 00:05:22,000\n",
            "打球是不是學習\n",
            "\n",
            "103\n",
            "00:05:22,000 --> 00:05:24,000\n",
            "當然是\n",
            "\n",
            "104\n",
            "00:05:24,000 --> 00:05:26,000\n",
            "在打球中間有沒有增長\n",
            "\n",
            "105\n",
            "00:05:26,000 --> 00:05:27,000\n",
            "當然是\n",
            "\n",
            "106\n",
            "00:05:27,000 --> 00:05:30,000\n",
            "在打球中間有沒有增長\n",
            "\n",
            "107\n",
            "00:05:30,000 --> 00:05:31,000\n",
            "當然有增長\n",
            "\n",
            "108\n",
            "00:05:31,000 --> 00:05:35,000\n",
            "打球不只是對健康有增長\n",
            "\n",
            "109\n",
            "00:05:35,000 --> 00:05:39,000\n",
            "而且可能對於譬如說手腦協調\n",
            "\n",
            "110\n",
            "00:05:39,000 --> 00:05:41,000\n",
            "譬如說團隊精神\n",
            "\n",
            "111\n",
            "00:05:41,000 --> 00:05:44,000\n",
            "譬如說個人之間的互動\n",
            "\n",
            "112\n",
            "00:05:44,000 --> 00:05:45,000\n",
            "什麼可能都有幫助\n",
            "\n",
            "113\n",
            "00:05:45,000 --> 00:05:47,000\n",
            "所以打球當然是有增長的\n",
            "\n",
            "114\n",
            "00:05:47,000 --> 00:05:50,000\n",
            "那當然是很好的學習的機會\n",
            "\n",
            "115\n",
            "00:05:50,000 --> 00:05:52,000\n",
            "有人喜歡爬山\n",
            "\n",
            "116\n",
            "00:05:52,000 --> 00:05:54,000\n",
            "爬山是不是好的學習機會\n",
            "\n",
            "117\n",
            "00:05:54,000 --> 00:05:55,000\n",
            "當然是\n",
            "\n",
            "118\n",
            "00:05:55,000 --> 00:05:57,000\n",
            "這個我以前兩年前就講過很多\n",
            "\n",
            "119\n",
            "00:05:57,000 --> 00:05:59,000\n",
            "爬山可以學到很多的\n",
            "\n",
            "120\n",
            "00:05:59,000 --> 00:06:02,000\n",
            "那爬山當然是一種學習\n",
            "\n",
            "121\n",
            "00:06:02,000 --> 00:06:03,000\n",
            "有人說我不喜歡爬山\n",
            "\n",
            "122\n",
            "00:06:03,000 --> 00:06:05,000\n",
            "我去旅行好不好\n",
            "\n",
            "123\n",
            "00:06:05,000 --> 00:06:07,000\n",
            "旅行當然好\n",
            "\n",
            "124\n",
            "00:06:07,000 --> 00:06:09,000\n",
            "旅行可以增長見識\n",
            "\n",
            "125\n",
            "00:06:09,000 --> 00:06:11,000\n",
            "可以擴增事業\n",
            "\n",
            "126\n",
            "00:06:11,000 --> 00:06:13,000\n",
            "可以增加很多很多\n",
            "\n",
            "127\n",
            "00:06:13,000 --> 00:06:14,000\n",
            "當然是有進步的\n",
            "\n",
            "128\n",
            "00:06:14,000 --> 00:06:16,000\n",
            "所以當然是很好的學習\n",
            "\n",
            "129\n",
            "00:06:16,000 --> 00:06:20,000\n",
            "你凡是獲得快樂都是很好的事\n",
            "\n",
            "130\n",
            "00:06:20,000 --> 00:06:23,000\n",
            "那這些都值得下功夫去\n",
            "\n",
            "131\n",
            "00:06:23,000 --> 00:06:25,000\n",
            "把它看成是學習\n",
            "\n",
            "132\n",
            "00:06:25,000 --> 00:06:27,000\n",
            "都值得下功夫去做的\n",
            "\n",
            "133\n",
            "00:06:27,000 --> 00:06:29,000\n",
            "我們再講另外一系列\n",
            "\n",
            "134\n",
            "00:06:29,000 --> 00:06:30,000\n",
            "譬如說\n",
            "\n",
            "135\n",
            "00:06:30,000 --> 00:06:34,000\n",
            "有人說談戀愛是不是學習\n",
            "\n",
            "136\n",
            "00:06:34,000 --> 00:06:37,000\n",
            "談戀愛除了你在談戀愛上\n",
            "\n",
            "137\n",
            "00:06:37,000 --> 00:06:39,000\n",
            "會有收穫以外\n",
            "\n",
            "138\n",
            "00:06:39,000 --> 00:06:41,000\n",
            "本身也是有收穫的\n",
            "\n",
            "139\n",
            "00:06:41,000 --> 00:06:44,000\n",
            "因為讓你體驗到人跟人之間\n",
            "\n",
            "140\n",
            "00:06:44,000 --> 00:06:45,000\n",
            "的各種感覺\n",
            "\n",
            "141\n",
            "00:06:45,000 --> 00:06:48,000\n",
            "人跟人之間的各種期待等等\n",
            "\n",
            "142\n",
            "00:06:48,000 --> 00:06:50,000\n",
            "有沒有幫助\n",
            "\n",
            "143\n",
            "00:06:50,000 --> 00:06:52,000\n",
            "當然有幫助\n",
            "\n",
            "144\n",
            "00:06:52,000 --> 00:06:54,000\n",
            "對每一個人都是很好的學習\n",
            "\n",
            "145\n",
            "00:06:54,000 --> 00:06:57,000\n",
            "所以談戀愛當然是一件很好的事\n",
            "\n",
            "146\n",
            "00:06:57,000 --> 00:07:00,000\n",
            "有人會說那要靠緣分\n",
            "\n",
            "147\n",
            "00:07:00,000 --> 00:07:02,000\n",
            "沒有緣分沒有辦法\n",
            "\n",
            "148\n",
            "00:07:02,000 --> 00:07:03,000\n",
            "對不對\n",
            "\n",
            "149\n",
            "00:07:03,000 --> 00:07:04,000\n",
            "對\n",
            "\n",
            "150\n",
            "00:07:04,000 --> 00:07:06,000\n",
            "但是你不是一定要談戀愛嗎\n",
            "\n",
            "151\n",
            "00:07:06,000 --> 00:07:07,000\n",
            "你可以交朋友\n",
            "\n",
            "152\n",
            "00:07:07,000 --> 00:07:10,000\n",
            "交朋友是不是學習\n",
            "\n",
            "153\n",
            "00:07:10,000 --> 00:07:11,000\n",
            "當然是\n",
            "\n",
            "154\n",
            "00:07:11,000 --> 00:07:13,000\n",
            "交朋友也一樣\n",
            "\n",
            "155\n",
            "00:07:13,000 --> 00:07:16,000\n",
            "讓我們學到很多人際的互動\n",
            "\n",
            "156\n",
            "00:07:16,000 --> 00:07:20,000\n",
            "學到很多人跟人之間的溝通\n",
            "\n",
            "157\n",
            "00:07:20,000 --> 00:07:22,000\n",
            "人跟人之間的期待\n",
            "\n",
            "158\n",
            "00:07:22,000 --> 00:07:24,000\n",
            "人跟人之間的感覺\n",
            "\n",
            "159\n",
            "00:07:24,000 --> 00:07:26,000\n",
            "這都是交朋友之後學到的\n",
            "\n",
            "160\n",
            "00:07:26,000 --> 00:07:28,000\n",
            "對我們電機系的同學而言\n",
            "\n",
            "161\n",
            "00:07:28,000 --> 00:07:31,000\n",
            "你四周有一大群好同學\n",
            "\n",
            "162\n",
            "00:07:31,000 --> 00:07:34,000\n",
            "都是很好的交朋友的對象\n",
            "\n",
            "163\n",
            "00:07:34,000 --> 00:07:36,000\n",
            "你下一番功夫交朋友好不好\n",
            "\n",
            "164\n",
            "00:07:36,000 --> 00:07:37,000\n",
            "好\n",
            "\n",
            "165\n",
            "00:07:37,000 --> 00:07:40,000\n",
            "當然是有幫助的\n",
            "\n",
            "166\n",
            "00:07:40,000 --> 00:07:42,000\n",
            "另外當然我們可以舉很多\n",
            "\n",
            "167\n",
            "00:07:42,000 --> 00:07:44,000\n",
            "我們最現成的例子\n",
            "\n",
            "168\n",
            "00:07:44,000 --> 00:07:47,000\n",
            "譬如說我們的戲學會辦各種活動\n",
            "\n",
            "169\n",
            "00:07:47,000 --> 00:07:49,000\n",
            "那些活動有沒有幫助\n",
            "\n",
            "170\n",
            "00:07:49,000 --> 00:07:50,000\n",
            "當然有\n",
            "\n",
            "171\n",
            "00:07:50,000 --> 00:07:54,000\n",
            "我們舉例來講電業\n",
            "\n",
            "172\n",
            "00:07:54,000 --> 00:07:57,000\n",
            "你如果去參加某一個舞跳個舞\n",
            "\n",
            "173\n",
            "00:07:57,000 --> 00:08:00,000\n",
            "或者參加某個劇演個劇\n",
            "\n",
            "174\n",
            "00:08:00,000 --> 00:08:01,000\n",
            "有沒有幫助\n",
            "\n",
            "175\n",
            "00:08:01,000 --> 00:08:02,000\n",
            "當然有幫助\n",
            "\n",
            "176\n",
            "00:08:02,000 --> 00:08:05,000\n",
            "你在這中間一定發現有所增長\n",
            "\n",
            "177\n",
            "00:08:05,000 --> 00:08:06,000\n",
            "有所進步\n",
            "\n",
            "178\n",
            "00:08:06,000 --> 00:08:09,000\n",
            "那是為什麼有那麼多同學要去參加\n",
            "\n",
            "179\n",
            "00:08:09,000 --> 00:08:13,000\n",
            "就是因為發現那個確實是有增長有進步\n",
            "\n",
            "180\n",
            "00:08:13,000 --> 00:08:15,000\n",
            "有的人說\n",
            "\n",
            "181\n",
            "00:08:15,000 --> 00:08:17,000\n",
            "我不去跳那個舞\n",
            "\n",
            "182\n",
            "00:08:17,000 --> 00:08:20,000\n",
            "或者演那個劇\n",
            "\n",
            "183\n",
            "00:08:20,000 --> 00:08:22,000\n",
            "我做幕後的\n",
            "\n",
            "184\n",
            "00:08:22,000 --> 00:08:25,000\n",
            "譬如說是幕後的什麼規劃\n",
            "\n",
            "185\n",
            "00:08:25,000 --> 00:08:29,000\n",
            "或者說是什麼光舞的什麼軟體組\n",
            "\n",
            "186\n",
            "00:08:29,000 --> 00:08:32,000\n",
            "還是什麼服裝道具組\n",
            "\n",
            "187\n",
            "00:08:32,000 --> 00:08:33,000\n",
            "一樣\n",
            "\n",
            "188\n",
            "00:08:33,000 --> 00:08:36,000\n",
            "那個都是可以有獲得很多的增長\n",
            "\n",
            "189\n",
            "00:08:36,000 --> 00:08:37,000\n",
            "很多進步的\n",
            "\n",
            "190\n",
            "00:08:37,000 --> 00:08:39,000\n",
            "當然都是很有用的\n",
            "\n",
            "191\n",
            "00:08:39,000 --> 00:08:42,000\n",
            "都是很好的學習\n",
            "\n",
            "192\n",
            "00:08:42,000 --> 00:08:47,000\n",
            "那當然也包括電業以外的戲學會\n",
            "\n",
            "193\n",
            "00:08:47,000 --> 00:08:50,000\n",
            "其他的各種活動都一樣\n",
            "\n",
            "194\n",
            "00:08:50,000 --> 00:08:55,000\n",
            "也包括電機系以外的其他的校內\n",
            "\n",
            "195\n",
            "00:08:55,000 --> 00:08:59,000\n",
            "或者校外的各種活動幾乎都一樣\n",
            "\n",
            "196\n",
            "00:08:59,000 --> 00:09:02,000\n",
            "都可以讓人有所增長有所進步\n",
            "\n",
            "197\n",
            "00:09:02,000 --> 00:09:04,000\n",
            "都是很好的學習的機會\n",
            "\n",
            "198\n",
            "00:09:04,000 --> 00:09:06,000\n",
            "都是很好的學習\n",
            "\n",
            "199\n",
            "00:09:06,000 --> 00:09:10,000\n",
            "同樣的問題是這些東西都沒有考試\n",
            "\n",
            "200\n",
            "00:09:10,000 --> 00:09:12,000\n",
            "沒有成績\n",
            "\n",
            "201\n",
            "00:09:12,000 --> 00:09:14,000\n",
            "不能顯示在成績單上\n",
            "\n",
            "202\n",
            "00:09:14,000 --> 00:09:18,000\n",
            "因此對有一些同學會認為那個浪費時間\n",
            "\n",
            "203\n",
            "00:09:18,000 --> 00:09:20,000\n",
            "我不需要花時間去做那個\n",
            "\n",
            "204\n",
            "00:09:20,000 --> 00:09:24,000\n",
            "因為不影響我的overfitting的目標\n",
            "\n",
            "205\n",
            "00:09:24,000 --> 00:09:26,000\n",
            "裡面沒有這個嘛\n",
            "\n",
            "206\n",
            "00:09:26,000 --> 00:09:28,000\n",
            "具體成績沒有這些嘛\n",
            "\n",
            "207\n",
            "00:09:28,000 --> 00:09:30,000\n",
            "那不要這樣想\n",
            "\n",
            "208\n",
            "00:09:30,000 --> 00:09:33,000\n",
            "因為那些都非常的重要\n",
            "\n",
            "209\n",
            "00:09:33,000 --> 00:09:36,000\n",
            "都對你發展非常的重要\n",
            "\n",
            "210\n",
            "00:09:36,000 --> 00:09:39,000\n",
            "那我們說電機工程\n",
            "\n",
            "211\n",
            "00:09:39,000 --> 00:09:41,000\n",
            "今天的電機工程\n",
            "\n",
            "212\n",
            "00:09:41,000 --> 00:09:44,000\n",
            "很少什麼事情自己一個人可以做成功的\n",
            "\n",
            "213\n",
            "00:09:44,000 --> 00:09:47,000\n",
            "你必須跟很多人一起\n",
            "\n",
            "214\n",
            "00:09:47,000 --> 00:09:50,000\n",
            "才可能做成功一個非常重要的\n",
            "\n",
            "215\n",
            "00:09:50,000 --> 00:09:52,000\n",
            "有意義的工作\n",
            "\n",
            "216\n",
            "00:09:52,000 --> 00:09:55,000\n",
            "那當你跟一群人在一起做的時候\n",
            "\n",
            "217\n",
            "00:09:55,000 --> 00:09:59,000\n",
            "你必須學會如何進入一個團隊\n",
            "\n",
            "218\n",
            "00:09:59,000 --> 00:10:03,000\n",
            "從邊緣開始慢慢進入核心\n",
            "\n",
            "219\n",
            "00:10:03,000 --> 00:10:06,000\n",
            "從底層開始慢慢變成leader\n",
            "\n",
            "220\n",
            "00:10:06,000 --> 00:10:09,000\n",
            "然後如何可以推動你想做的事\n",
            "\n",
            "221\n",
            "00:10:09,000 --> 00:10:13,000\n",
            "如何變成可以做到你想做的事等等\n",
            "\n",
            "222\n",
            "00:10:13,000 --> 00:10:15,000\n",
            "這些都是很重要的\n",
            "\n",
            "223\n",
            "00:10:15,000 --> 00:10:18,000\n",
            "那我們通常稱這些東西\n",
            "\n",
            "224\n",
            "00:10:18,000 --> 00:10:21,000\n",
            "是所謂的soft skills\n",
            "\n",
            "225\n",
            "00:10:21,000 --> 00:10:24,000\n",
            "也就是軟實力\n",
            "\n",
            "226\n",
            "00:10:24,000 --> 00:10:30,000\n",
            "所謂軟實力就是硬實力以外的軟實力\n",
            "\n",
            "227\n",
            "00:10:30,000 --> 00:10:34,000\n",
            "硬實力是說你電子學的功力\n",
            "\n",
            "228\n",
            "00:10:34,000 --> 00:10:36,000\n",
            "數學的功力\n",
            "\n",
            "229\n",
            "00:10:36,000 --> 00:10:40,000\n",
            "這個城市能力這種是硬實力\n",
            "\n",
            "230\n",
            "00:10:40,000 --> 00:10:45,000\n",
            "軟實力我們主要就是講各種人際之間的\n",
            "\n",
            "231\n",
            "00:10:45,000 --> 00:10:49,000\n",
            "在人跟人之間的各種能力\n",
            "\n",
            "232\n",
            "00:10:49,000 --> 00:10:54,000\n",
            "包括溝通能力協調能力交朋友的能力\n",
            "\n",
            "233\n",
            "00:10:54,000 --> 00:10:59,000\n",
            "說服人的能力團隊精神領導能力等等\n",
            "\n",
            "234\n",
            "00:10:59,000 --> 00:11:02,000\n",
            "那些就是所謂的soft skills\n",
            "\n",
            "235\n",
            "00:11:02,000 --> 00:11:04,000\n",
            "重要不重要重要\n",
            "\n",
            "236\n",
            "00:11:04,000 --> 00:11:07,000\n",
            "你看到任何一個成功的電機工程師\n",
            "\n",
            "237\n",
            "00:11:07,000 --> 00:11:09,000\n",
            "他都有一堆這種\n",
            "\n",
            "238\n",
            "00:11:09,000 --> 00:11:13,000\n",
            "這個才是他成功的一個非常重要的關鍵\n",
            "\n",
            "239\n",
            "00:11:13,000 --> 00:11:15,000\n",
            "這種東西怎麼來\n",
            "\n",
            "240\n",
            "00:11:15,000 --> 00:11:18,000\n",
            "我們剛才講的各種課業外的\n",
            "\n",
            "241\n",
            "00:11:18,000 --> 00:11:20,000\n",
            "各種學習增長的機會\n",
            "\n",
            "242\n",
            "00:11:20,000 --> 00:11:26,000\n",
            "都可以幫助一個人塑造他的soft skills\n",
            "\n",
            "243\n",
            "00:11:26,000 --> 00:11:30,000\n",
            "是有少數人的這些soft skills是天生的\n",
            "\n",
            "244\n",
            "00:11:30,000 --> 00:11:31,000\n",
            "他天生就厲害\n",
            "\n",
            "245\n",
            "00:11:31,000 --> 00:11:32,000\n",
            "有沒有\n",
            "\n",
            "246\n",
            "00:11:32,000 --> 00:11:33,000\n",
            "有\n",
            "\n",
            "247\n",
            "00:11:33,000 --> 00:11:35,000\n",
            "但這種人畢竟沒那麼多\n",
            "\n",
            "248\n",
            "00:11:35,000 --> 00:11:37,000\n",
            "對很多人而言\n",
            "\n",
            "249\n",
            "00:11:37,000 --> 00:11:42,000\n",
            "他的soft skills是自己努力慢慢培養起來的\n",
            "\n",
            "250\n",
            "00:11:42,000 --> 00:11:45,000\n",
            "我剛才一開始前面講的那一段\n",
            "\n",
            "251\n",
            "00:11:45,000 --> 00:11:49,000\n",
            "我說我在進台大電機系以前\n",
            "\n",
            "252\n",
            "00:11:49,000 --> 00:11:51,000\n",
            "我幾乎不會交朋友\n",
            "\n",
            "253\n",
            "00:11:51,000 --> 00:11:53,000\n",
            "我不太會說話\n",
            "\n",
            "254\n",
            "00:11:53,000 --> 00:11:57,000\n",
            "我在讀大學的四年裡面改變我自己\n",
            "\n",
            "255\n",
            "00:11:57,000 --> 00:12:02,000\n",
            "讓我變成有很多這方面的能力的人\n",
            "\n",
            "256\n",
            "00:12:02,000 --> 00:12:06,000\n",
            "其實最重要就是我的很多soft skills\n",
            "\n",
            "257\n",
            "00:12:06,000 --> 00:12:07,000\n",
            "都是我自己培養\n",
            "\n",
            "258\n",
            "00:12:07,000 --> 00:12:10,000\n",
            "在讀台大電機系的四年裡面\n",
            "\n",
            "259\n",
            "00:12:10,000 --> 00:12:14,000\n",
            "獲得的非常多這方面的收穫的\n",
            "\n",
            "260\n",
            "00:12:14,000 --> 00:12:18,000\n",
            "那是為什麼我每次都要強調\n",
            "\n",
            "261\n",
            "00:12:18,000 --> 00:12:21,000\n",
            "這個東西有多麼重要\n",
            "\n",
            "262\n",
            "00:12:21,000 --> 00:12:27,000\n",
            "我之前曾經在幾年前的這個\n",
            "\n",
            "263\n",
            "00:12:27,000 --> 00:12:31,000\n",
            "信號與人生裡面有說到這一件事\n",
            "\n",
            "264\n",
            "00:12:31,000 --> 00:12:33,000\n",
            "我現在不要重複\n",
            "\n",
            "265\n",
            "00:12:33,000 --> 00:12:36,000\n",
            "但是我簡單的summarize\n",
            "\n",
            "266\n",
            "00:12:36,000 --> 00:12:42,000\n",
            "我說我們電機系的電機工程師的\n",
            "\n",
            "267\n",
            "00:12:42,000 --> 00:12:45,000\n",
            "一生career的發展\n",
            "\n",
            "268\n",
            "00:12:45,000 --> 00:12:48,000\n",
            "黃金實在是在什麼時候\n",
            "\n",
            "269\n",
            "00:12:48,000 --> 00:12:52,000\n",
            "我認為是在35歲到55歲\n",
            "\n",
            "270\n",
            "00:12:52,000 --> 00:12:56,000\n",
            "這20年是我們的黃金時代\n",
            "\n",
            "271\n",
            "00:12:56,000 --> 00:12:58,000\n",
            "在這以前當然更好\n",
            "\n",
            "272\n",
            "00:12:58,000 --> 00:13:01,000\n",
            "只是說可能各方面尚未具備\n",
            "\n",
            "273\n",
            "00:13:01,000 --> 00:13:03,000\n",
            "還沒有完全訓練的好\n",
            "\n",
            "274\n",
            "00:13:03,000 --> 00:13:05,000\n",
            "在這以後是最好的\n",
            "\n",
            "275\n",
            "00:13:05,000 --> 00:13:10,000\n",
            "這以後年紀大了難免有一些要打個折扣等等\n",
            "\n",
            "276\n",
            "00:13:10,000 --> 00:13:13,000\n",
            "就這裡面我們看到\n",
            "\n",
            "277\n",
            "00:13:13,000 --> 00:13:15,000\n",
            "我們的電機系的畢業的同學\n",
            "\n",
            "278\n",
            "00:13:15,000 --> 00:13:18,000\n",
            "過去有幾千人畢業我都看到\n",
            "\n",
            "279\n",
            "00:13:18,000 --> 00:13:22,000\n",
            "我覺得有的人的發展是像這樣\n",
            "\n",
            "280\n",
            "00:13:22,000 --> 00:13:24,000\n",
            "有一定的斜率\n",
            "\n",
            "281\n",
            "00:13:24,000 --> 00:13:28,000\n",
            "但到某一個階段它會慢慢saturate\n",
            "\n",
            "282\n",
            "00:13:28,000 --> 00:13:31,000\n",
            "有的人也許開始向上比較晚\n",
            "\n",
            "283\n",
            "00:13:31,000 --> 00:13:33,000\n",
            "但它斜率比較高\n",
            "\n",
            "284\n",
            "00:13:33,000 --> 00:13:38,000\n",
            "它最後會saturate在比較高的地方\n",
            "\n",
            "285\n",
            "00:13:38,000 --> 00:13:41,000\n",
            "也有的人也許開始的比較快\n",
            "\n",
            "286\n",
            "00:13:41,000 --> 00:13:44,000\n",
            "但是後來會overshoot之後\n",
            "\n",
            "287\n",
            "00:13:44,000 --> 00:13:46,000\n",
            "會開始收斂在比較低的地方等等\n",
            "\n",
            "288\n",
            "00:13:46,000 --> 00:13:48,000\n",
            "每一個人都不一樣\n",
            "\n",
            "289\n",
            "00:13:48,000 --> 00:13:50,000\n",
            "但是當然也有一種人\n",
            "\n",
            "290\n",
            "00:13:50,000 --> 00:13:54,000\n",
            "你會看到他一直向上走\n",
            "\n",
            "291\n",
            "00:13:54,000 --> 00:13:58,000\n",
            "完全沒有saturate\n",
            "\n",
            "292\n",
            "00:13:58,000 --> 00:14:01,000\n",
            "這些人這些差別在哪裡\n",
            "\n",
            "293\n",
            "00:14:01,000 --> 00:14:04,000\n",
            "這些東西差別在哪裡\n",
            "\n",
            "294\n",
            "00:14:04,000 --> 00:14:06,000\n",
            "我以前已經說過這件事\n",
            "\n",
            "295\n",
            "00:14:06,000 --> 00:14:08,000\n",
            "我不要多重複\n",
            "\n",
            "296\n",
            "00:14:08,000 --> 00:14:11,000\n",
            "我說最主要因素有四個\n",
            "\n",
            "297\n",
            "00:14:11,000 --> 00:14:18,000\n",
            "實力、努力、大智\n",
            "\n",
            "298\n",
            "00:14:18,000 --> 00:14:23,000\n",
            "跟self skill這四件事情\n",
            "\n",
            "299\n",
            "00:14:23,000 --> 00:14:29,000\n",
            "我認為真正影響這個的\n",
            "\n",
            "300\n",
            "00:14:29,000 --> 00:14:32,000\n",
            "不是因為電子學考得好不好\n",
            "\n",
            "301\n",
            "00:14:32,000 --> 00:14:35,000\n",
            "不是因為信號與系統念得好不好\n",
            "\n",
            "302\n",
            "00:14:35,000 --> 00:14:37,000\n",
            "也就是我剛才講\n",
            "\n",
            "303\n",
            "00:14:37,000 --> 00:14:39,000\n",
            "你把每一門必修課\n",
            "\n",
            "304\n",
            "00:14:39,000 --> 00:14:41,000\n",
            "當成是單一跑道\n",
            "\n",
            "305\n",
            "00:14:41,000 --> 00:14:43,000\n",
            "跑到第一名並不表示怎樣\n",
            "\n",
            "306\n",
            "00:14:43,000 --> 00:14:45,000\n",
            "我們最後不看那個的\n",
            "\n",
            "307\n",
            "00:14:45,000 --> 00:14:47,000\n",
            "最後看的是這個\n",
            "\n",
            "308\n",
            "00:14:47,000 --> 00:14:49,000\n",
            "這個是怎麼樣影響\n",
            "\n",
            "309\n",
            "00:14:49,000 --> 00:14:51,000\n",
            "我認為是這四件事\n",
            "\n",
            "310\n",
            "00:14:51,000 --> 00:14:55,000\n",
            "就是實力、努力、大智跟self skills\n",
            "\n",
            "311\n",
            "00:14:55,000 --> 00:14:57,000\n",
            "這四件事裡面\n",
            "\n",
            "312\n",
            "00:14:57,000 --> 00:14:59,000\n",
            "我們現在可以summarize\n",
            "\n",
            "313\n",
            "00:14:59,000 --> 00:15:00,000\n",
            "我剛才講的\n",
            "\n",
            "314\n",
            "00:15:00,000 --> 00:15:02,000\n",
            "什麼是實力\n",
            "\n",
            "315\n",
            "00:15:02,000 --> 00:15:05,000\n",
            "實力就是所有的這些\n",
            "\n",
            "316\n",
            "00:15:05,000 --> 00:15:07,000\n",
            "我們電機工程的專業領域裡面\n",
            "\n",
            "317\n",
            "00:15:07,000 --> 00:15:09,000\n",
            "各種東西的實力\n",
            "\n",
            "318\n",
            "00:15:09,000 --> 00:15:11,000\n",
            "實力怎麼厲害法\n",
            "\n",
            "319\n",
            "00:15:11,000 --> 00:15:13,000\n",
            "就是我剛才講的\n",
            "\n",
            "320\n",
            "00:15:13,000 --> 00:15:16,000\n",
            "你如果都是在做全面的學習的話\n",
            "\n",
            "321\n",
            "00:15:16,000 --> 00:15:18,000\n",
            "你就會學到各種該學到的\n",
            "\n",
            "322\n",
            "00:15:18,000 --> 00:15:20,000\n",
            "最後你的實力就是很強的\n",
            "\n",
            "323\n",
            "00:15:20,000 --> 00:15:25,000\n",
            "所以實力最主要就是不要overfitting\n",
            "\n",
            "324\n",
            "00:15:25,000 --> 00:15:27,000\n",
            "要盡量都做\n",
            "\n",
            "325\n",
            "00:15:27,000 --> 00:15:31,000\n",
            "學到該學的全面的學習\n",
            "\n",
            "326\n",
            "00:15:31,000 --> 00:15:33,000\n",
            "努力是沒有疑問\n",
            "\n",
            "327\n",
            "00:15:33,000 --> 00:15:34,000\n",
            "每一個人都了解\n",
            "\n",
            "328\n",
            "00:15:34,000 --> 00:15:36,000\n",
            "確實我們可以看到\n",
            "\n",
            "329\n",
            "00:15:36,000 --> 00:15:38,000\n",
            "一個人在未來的幾十年裡面\n",
            "\n",
            "330\n",
            "00:15:38,000 --> 00:15:40,000\n",
            "有的人他一直努力\n",
            "\n",
            "331\n",
            "00:15:40,000 --> 00:15:42,000\n",
            "有的人慢慢不太努力等等\n",
            "\n",
            "332\n",
            "00:15:42,000 --> 00:15:44,000\n",
            "這個是有明顯差別的\n",
            "\n",
            "333\n",
            "00:15:44,000 --> 00:15:46,000\n",
            "那self skills我剛才已經講了\n",
            "\n",
            "334\n",
            "00:15:46,000 --> 00:15:50,000\n",
            "就是很多我們平常沒有算成績\n",
            "\n",
            "335\n",
            "00:15:50,000 --> 00:15:52,000\n",
            "覺得大家不重視的事情\n",
            "\n",
            "336\n",
            "00:15:52,000 --> 00:15:54,000\n",
            "其實它常常是很重要的\n",
            "\n",
            "337\n",
            "00:15:54,000 --> 00:15:56,000\n",
            "你如果好好的\n",
            "\n",
            "338\n",
            "00:15:56,000 --> 00:16:00,000\n",
            "多在各種課業外的事情上\n",
            "\n",
            "339\n",
            "00:16:00,000 --> 00:16:02,000\n",
            "增長進步的話\n",
            "\n",
            "340\n",
            "00:16:02,000 --> 00:16:04,000\n",
            "你這些東西會很強\n",
            "\n",
            "341\n",
            "00:16:04,000 --> 00:16:06,000\n",
            "你會很厲害的\n",
            "\n",
            "342\n",
            "00:16:06,000 --> 00:16:08,000\n",
            "當然對少數人而言\n",
            "\n",
            "343\n",
            "00:16:08,000 --> 00:16:09,000\n",
            "他天生就有\n",
            "\n",
            "344\n",
            "00:16:09,000 --> 00:16:10,000\n",
            "他可能不需要\n",
            "\n",
            "345\n",
            "00:16:10,000 --> 00:16:12,000\n",
            "這是每一個人不一樣的\n",
            "\n",
            "346\n",
            "00:16:12,000 --> 00:16:14,000\n",
            "那這三個我都提過了\n",
            "\n",
            "347\n",
            "00:16:14,000 --> 00:16:16,000\n",
            "那麼大致我還沒有提\n",
            "\n",
            "348\n",
            "00:16:16,000 --> 00:16:18,000\n",
            "其實大致沒有什麼要特別說的\n",
            "\n",
            "349\n",
            "00:16:18,000 --> 00:16:21,000\n",
            "那應該就是我剛才前面有講過\n",
            "\n",
            "350\n",
            "00:16:21,000 --> 00:16:26,000\n",
            "就是每一個人可以有你自己的長程目標\n",
            "\n",
            "351\n",
            "00:16:26,000 --> 00:16:29,000\n",
            "那有的人本來就有了\n",
            "\n",
            "352\n",
            "00:16:29,000 --> 00:16:32,000\n",
            "有的人也許我平常沒有想過\n",
            "\n",
            "353\n",
            "00:16:32,000 --> 00:16:37,000\n",
            "那你可以在適當時機開始想\n",
            "\n",
            "354\n",
            "00:16:37,000 --> 00:16:40,000\n",
            "我有沒有想要做什麼事情\n",
            "\n",
            "355\n",
            "00:16:40,000 --> 00:16:43,000\n",
            "哪些事情可能是我的長程目標\n",
            "\n",
            "356\n",
            "00:16:43,000 --> 00:16:48,000\n",
            "我希望最後讓我花個5年10年\n",
            "\n",
            "357\n",
            "00:16:48,000 --> 00:16:50,000\n",
            "15年或者更長\n",
            "\n",
            "358\n",
            "00:16:50,000 --> 00:16:52,000\n",
            "我把我的很多的努力\n",
            "\n",
            "359\n",
            "00:16:52,000 --> 00:16:55,000\n",
            "都來把某一些事情做得非常漂亮\n",
            "\n",
            "360\n",
            "00:16:55,000 --> 00:16:57,000\n",
            "那是我很想做的事\n",
            "\n",
            "361\n",
            "00:16:57,000 --> 00:17:00,000\n",
            "那就是長程目標\n",
            "\n",
            "362\n",
            "00:17:00,000 --> 00:17:04,000\n",
            "如果我覺得做那些事情會讓我非常的\n",
            "\n",
            "363\n",
            "00:17:04,000 --> 00:17:05,000\n",
            "覺得有意義\n",
            "\n",
            "364\n",
            "00:17:05,000 --> 00:17:07,000\n",
            "願意花功夫下去做的\n",
            "\n",
            "365\n",
            "00:17:07,000 --> 00:17:09,000\n",
            "那就是我的長程目標\n",
            "\n",
            "366\n",
            "00:17:09,000 --> 00:17:12,000\n",
            "那有的人如果可以想出這個來的話\n",
            "\n",
            "367\n",
            "00:17:12,000 --> 00:17:15,000\n",
            "那就是他的大致\n",
            "\n",
            "368\n",
            "00:17:15,000 --> 00:17:17,000\n",
            "那越是有這種大致的人\n",
            "\n",
            "369\n",
            "00:17:17,000 --> 00:17:20,000\n",
            "也比較容易向上衝\n",
            "\n",
            "370\n",
            "00:17:20,000 --> 00:17:25,000\n",
            "那我感覺起來真正影響的就是這四件事\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "''' Open the SRT file and read its content.\n",
        "The format of SRT is:\n",
        "\n",
        "[Index]\n",
        "[Begin time] (hour:minute:second) --> [End time] (hour:minute:second)\n",
        "[Transcription]\n",
        "\n",
        "'''\n",
        "\n",
        "with open(output_subtitle_path, 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7JcN-kUDE_g"
      },
      "source": [
        "# Part3 - Preprocess the results of automatic speech recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P2R40faVDShf"
      },
      "outputs": [],
      "source": [
        "def extract_and_save_text(srt_filename, output_filename):\n",
        "\n",
        "    '''\n",
        "    (1) Objective:\n",
        "        - This function extracts the text from an SRT file and saves it to a new text file.\n",
        "        - It also converts the Simplified Chinese to Traditional Chinese.\n",
        "\n",
        "    (2) Arguments:\n",
        "\n",
        "        - srt_filename: The path to the SRT file.\n",
        "\n",
        "        - output_filename: The name of the output text file.\n",
        "\n",
        "    (3) Example:\n",
        "        - If your SRT file is named 'subtitle.srt' and you want to save the extracted text to a file named 'output.txt', you can use the function like this:\n",
        "            extract_and_save_text('subtitle.srt', 'output.txt')\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Open the SRT file and read its content.\n",
        "    with open(srt_filename, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Use regular expression to remove the timecode.\n",
        "    pure_text = re.sub(r'\\d+\\n\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}\\n', '', content)\n",
        "\n",
        "    # Remove the empty lines.\n",
        "    pure_text = re.sub(r'\\n\\n+', '\\n', pure_text)\n",
        "\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    pure_text_conversion = cc.convert(pure_text)\n",
        "\n",
        "    # Write the extracted text to a new file.\n",
        "    with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
        "        output_file.write(pure_text_conversion)\n",
        "\n",
        "    print(f'Extracted text has been saved to {output_filename}.\\n\\n')\n",
        "\n",
        "    return pure_text_conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tWDl1vuADd0e"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, max_length):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - This function is used to split a long string into smaller strings of a specified length.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - text: str, the long string to be split.\n",
        "        - max_length: int, the maximum length of each smaller string.\n",
        "\n",
        "    (3) Returns:\n",
        "        - split_text: list, a list of smaller strings.\n",
        "\n",
        "    (3) Example:\n",
        "        - If you want to split a string named \"long_string\" into smaller strings of length 100, you can use the function like this:\n",
        "            chunk_text(long_string, 100)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return textwrap.wrap(text, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aO01S41pOsSP"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired parameters and the path of input file. '''\n",
        "\n",
        "# # The length of the text chunks.\n",
        "chunk_length = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O-PpbkoS-5bI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc50d961-3c34-436d-83d0-9b351fb73127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text has been saved to ./output-信號與人生.txt.\n",
            "\n",
            "\n",
            "Review the results of splitting the long text into several short texts.\n",
            "\n",
            "\n",
            "========== The 1-st segment of the split (505 words) ==========\n",
            "\n",
            "\n",
            "每次說這個學問是做出來的 什麼意思? 要做才會獲得學問 你如果每天光是坐在那裡聽 學問很可能是左耳進右耳出的 你光是坐在那兒讀\n",
            "\n",
            "學問可能從眼睛進入腦海之後就忘掉了 如何能夠學問在腦海裡面 真的變成你自己學問 就是要做 可能有很多同學有這個經驗 你如果去修某一門課 或者做某一個實驗\n",
            "\n",
            "在期末就是要教一個final project 那個final project就是要你把 學到的很多東西 最後整合在你的final project裡面\n",
            "\n",
            "最後做出來的時候 就是把它們都整合了 當你學期結束 真的把final project做完的時候 你會忽然發現 我真的學到很多東西 那就是做出來的學問\n",
            "\n",
            "也許可以舉另外一個例子 就是你如果學了某一些很複雜的演算法 或者什麼 好像覺得那些不見得在你的腦海裡 可是後來老師出了個習題 那個習題教你寫一個很大的程式\n",
            "\n",
            "要把所有東西都包進去 當你把這個程式寫完的時候你會發現 你忽然把演算法裡所有東西都弄通了 那就是學問是做出來的 所以我們永遠要記得 盡量多動手多做\n",
            "\n",
            "在動手跟做的過程之中 學問纔可以變成是自己的 同樣的情形就是說 很多時候這樣動手或者做的表現或者成績 沒有一個成績單上的數字\n",
            "\n",
            "\n",
            "========== The 2-nd segment of the split (506 words) ==========\n",
            "\n",
            "\n",
            "使得很多人覺得那不重要 很多人甚至覺得這門課要做final project 我就不修了太累了 或者說那門課需要怎麼樣怎麼樣太累 我就不要做了\n",
            "\n",
            "而不知道其實那個纔是讓你做的機會 然後可以學到最多 也就是說雖然很可能那麼辛苦的做很多事 沒有讓你獲得什麼具體成績 對你的overfitting可能沒有幫助\n",
            "\n",
            "可是對你的全面學習是很有幫助 是該學的 那不要漏掉這些事 那這是我所說的 那這個課業內可以做的這些事 那剛才我們講到思考的時候 我覺得我漏掉一點\n",
            "\n",
            "你如果修我的信號課你可能會發現 我上課沒講到一個數學式子的時候 我通常都不推他的 我是在解釋那個數學式子在說什麼話 同樣的呢 沒講到一個什麼什麼事情的時候\n",
            "\n",
            "我通常就在解釋他在說什麼話 也就是說 我在講的就是我讀到特本那裡的時候 我心裡怎麼想的 也就是我在告訴同學如何這個讀書的時候 如何一面讀一面練習思考\n",
            "\n",
            "那這個纔是最重要的一件事 如何培養自己思考的能力 跟培養思考的習慣 我覺得最好的辦法就是讀書的時候 凡是讀到一個數學式子都去想一想 那個數學式子到底在說什麼\n",
            "\n",
            "凡是讀到特本上講什麼就去想一想 那個到底在說什麼 你要真的瞭解他在說什麼的時候 你就用了很多思考的功夫\n",
            "\n",
            "\n",
            "========== The 3-rd segment of the split (504 words) ==========\n",
            "\n",
            "\n",
            "你就在練習自己思考的能力了 好 以上說的是課業內的部分 那當然除了課業內之外呢 還有一大堆是不在課業內的 那就是課業外的 課業外也有很多式的 那我們可以舉例來說\n",
            "\n",
            "課業外有什麼可以學習的 那我通常把學習定義成為 什麼是學習 學習就是一種增長 一種進步 然後獲得快樂 這就是學習 所以即使是課業外的任何事情\n",
            "\n",
            "只要你覺得是有增長的 是有進步的 讓你覺得快樂的 那應該就是值得學習的地方 那我們可以舉很多例子 譬如說很多同學喜歡打球 打球是不是學習 當然是\n",
            "\n",
            "在打球中間有沒有增長 當然是 在打球中間有沒有增長 當然有增長 打球不只是對健康有增長 而且可能對於譬如說手腦協調 譬如說團隊精神 譬如說個人之間的互動\n",
            "\n",
            "什麼可能都有幫助 所以打球當然是有增長的 那當然是很好的學習的機會 有人喜歡爬山 爬山是不是好的學習機會 當然是 這個我以前兩年前就講過很多 爬山可以學到很多的\n",
            "\n",
            "那爬山當然是一種學習 有人說我不喜歡爬山 我去旅行好不好 旅行當然好 旅行可以增長見識 可以擴增事業 可以增加很多很多 當然是有進步的 所以當然是很好的學習\n",
            "\n",
            "你凡是獲得快樂都是很好的事 那這些都值得下功夫去 把它看成是學習 都值得下功夫去做的\n",
            "\n",
            "\n",
            "========== The 4-th segment of the split (506 words) ==========\n",
            "\n",
            "\n",
            "我們再講另外一系列 譬如說 有人說談戀愛是不是學習 談戀愛除了你在談戀愛上 會有收穫以外 本身也是有收穫的 因為讓你體驗到人跟人之間 的各種感覺\n",
            "\n",
            "人跟人之間的各種期待等等 有沒有幫助 當然有幫助 對每一個人都是很好的學習 所以談戀愛當然是一件很好的事 有人會說那要靠緣分 沒有緣分沒有辦法 對不對 對\n",
            "\n",
            "但是你不是一定要談戀愛嗎 你可以交朋友 交朋友是不是學習 當然是 交朋友也一樣 讓我們學到很多人際的互動 學到很多人跟人之間的溝通 人跟人之間的期待\n",
            "\n",
            "人跟人之間的感覺 這都是交朋友之後學到的 對我們電機系的同學而言 你四周有一大羣好同學 都是很好的交朋友的對象 你下一番功夫交朋友好不好 好 當然是有幫助的\n",
            "\n",
            "另外當然我們可以舉很多 我們最現成的例子 譬如說我們的戲學會辦各種活動 那些活動有沒有幫助 當然有 我們舉例來講電業 你如果去參加某一個舞跳個舞\n",
            "\n",
            "或者參加某個劇演個劇 有沒有幫助 當然有幫助 你在這中間一定發現有所增長 有所進步 那是為什麼有那麼多同學要去參加 就是因為發現那個確實是有增長有進步 有的人說\n",
            "\n",
            "我不去跳那個舞 或者演那個劇 我做幕後的 譬如說是幕後的什麼規劃 或者說是什麼光舞的什麼軟體組\n",
            "\n",
            "\n",
            "========== The 5-th segment of the split (501 words) ==========\n",
            "\n",
            "\n",
            "還是什麼服裝道具組 一樣 那個都是可以有獲得很多的增長 很多進步的 當然都是很有用的 都是很好的學習 那當然也包括電業以外的戲學會 其他的各種活動都一樣\n",
            "\n",
            "也包括電機系以外的其他的校內 或者校外的各種活動幾乎都一樣 都可以讓人有所增長有所進步 都是很好的學習的機會 都是很好的學習 同樣的問題是這些東西都沒有考試\n",
            "\n",
            "沒有成績 不能顯示在成績單上 因此對有一些同學會認為那個浪費時間 我不需要花時間去做那個 因為不影響我的overfitting的目標 裡面沒有這個嘛\n",
            "\n",
            "具體成績沒有這些嘛 那不要這樣想 因為那些都非常的重要 都對你發展非常的重要 那我們說電機工程 今天的電機工程 很少什麼事情自己一個人可以做成功的\n",
            "\n",
            "你必須跟很多人一起 纔可能做成功一個非常重要的 有意義的工作 那當你跟一羣人在一起做的時候 你必須學會如何進入一個團隊 從邊緣開始慢慢進入核心\n",
            "\n",
            "從底層開始慢慢變成leader 然後如何可以推動你想做的事 如何變成可以做到你想做的事等等 這些都是很重要的 那我們通常稱這些東西 是所謂的soft\n",
            "\n",
            "skills 也就是軟實力 所謂軟實力就是硬實力以外的軟實力 硬實力是說你電子學的功力 數學的功力\n",
            "\n",
            "\n",
            "========== The 6-th segment of the split (504 words) ==========\n",
            "\n",
            "\n",
            "這個城市能力這種是硬實力 軟實力我們主要就是講各種人際之間的 在人跟人之間的各種能力 包括溝通能力協調能力交朋友的能力 說服人的能力團隊精神領導能力等等\n",
            "\n",
            "那些就是所謂的soft skills 重要不重要重要 你看到任何一個成功的電機工程師 他都有一堆這種 這個纔是他成功的一個非常重要的關鍵 這種東西怎麼來\n",
            "\n",
            "我們剛才講的各種課業外的 各種學習增長的機會 都可以幫助一個人塑造他的soft skills 是有少數人的這些soft skills是天生的 他天生就厲害\n",
            "\n",
            "有沒有 有 但這種人畢竟沒那麼多 對很多人而言 他的soft skills是自己努力慢慢培養起來的 我剛才一開始前面講的那一段 我說我在進臺大電機系以前\n",
            "\n",
            "我幾乎不會交朋友 我不太會說話 我在讀大學的四年裡面改變我自己 讓我變成有很多這方面的能力的人 其實最重要就是我的很多soft skills 都是我自己培養\n",
            "\n",
            "在讀臺大電機系的四年裡面 獲得的非常多這方面的收穫的 那是為什麼我每次都要強調 這個東西有多麼重要 我之前曾經在幾年前的這個 信號與人生裡面有說到這一件事\n",
            "\n",
            "我現在不要重複 但是我簡單的summarize 我說我們電機系的電機工程師的\n",
            "\n",
            "\n",
            "========== The 7-th segment of the split (502 words) ==========\n",
            "\n",
            "\n",
            "一生career的發展 黃金實在是在什麼時候 我認為是在35歲到55歲 這20年是我們的黃金時代 在這以前當然更好 只是說可能各方面尚未具備 還沒有完全訓練的好\n",
            "\n",
            "在這以後是最好的 這以後年紀大了難免有一些要打個折扣等等 就這裡面我們看到 我們的電機系的畢業的同學 過去有幾千人畢業我都看到 我覺得有的人的發展是像這樣\n",
            "\n",
            "有一定的斜率 但到某一個階段它會慢慢saturate 有的人也許開始向上比較晚 但它斜率比較高 它最後會saturate在比較高的地方 也有的人也許開始的比較快\n",
            "\n",
            "但是後來會overshoot之後 會開始收斂在比較低的地方等等 每一個人都不一樣 但是當然也有一種人 你會看到他一直向上走 完全沒有saturate\n",
            "\n",
            "這些人這些差別在哪裡 這些東西差別在哪裡 我以前已經說過這件事 我不要多重複 我說最主要因素有四個 實力、努力、大智 跟self skill這四件事情\n",
            "\n",
            "我認為真正影響這個的 不是因為電子學考得好不好 不是因為信號與系統念得好不好 也就是我剛才講 你把每一門必修課 當成是單一跑道 跑到第一名並不表示怎樣\n",
            "\n",
            "我們最後不看那個的 最後看的是這個 這個是怎麼樣影響 我認為是這四件事\n",
            "\n",
            "\n",
            "========== The 8-th segment of the split (512 words) ==========\n",
            "\n",
            "\n",
            "就是實力、努力、大智跟self skills 這四件事裡面 我們現在可以summarize 我剛才講的 什麼是實力 實力就是所有的這些\n",
            "\n",
            "我們電機工程的專業領域裡面 各種東西的實力 實力怎麼厲害法 就是我剛才講的 你如果都是在做全面的學習的話 你就會學到各種該學到的 最後你的實力就是很強的\n",
            "\n",
            "所以實力最主要就是不要overfitting 要盡量都做 學到該學的全面的學習 努力是沒有疑問 每一個人都瞭解 確實我們可以看到 一個人在未來的幾十年裡面\n",
            "\n",
            "有的人他一直努力 有的人慢慢不太努力等等 這個是有明顯差別的 那self skills我剛才已經講了 就是很多我們平常沒有算成績 覺得大家不重視的事情\n",
            "\n",
            "其實它常常是很重要的 你如果好好的 多在各種課業外的事情上 增長進步的話 你這些東西會很強 你會很厲害的 當然對少數人而言 他天生就有 他可能不需要\n",
            "\n",
            "這是每一個人不一樣的 那這三個我都提過了 那麼大致我還沒有提 其實大致沒有什麼要特別說的 那應該就是我剛才前面有講過 就是每一個人可以有你自己的長程目標\n",
            "\n",
            "那有的人本來就有了 有的人也許我平常沒有想過 那你可以在適當時機開始想 我有沒有想要做什麼事情 哪些事情可能是我的長程目標\n",
            "\n",
            "\n",
            "========== The 9-th segment of the split (169 words) ==========\n",
            "\n",
            "\n",
            "我希望最後讓我花個5年10年 15年或者更長 我把我的很多的努力 都來把某一些事情做得非常漂亮 那是我很想做的事 那就是長程目標\n",
            "\n",
            "如果我覺得做那些事情會讓我非常的 覺得有意義 願意花功夫下去做的 那就是我的長程目標 那有的人如果可以想出這個來的話 那就是他的大致 那越是有這種大致的人\n",
            "\n",
            "也比較容易向上衝 那我感覺起來真正影響的就是這四件事\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extracts the text from an SRT file and saves it to a new text file\n",
        "pure_text = extract_and_save_text(srt_filename=output_subtitle_path, output_filename=output_raw_text_path)\n",
        "\n",
        "# Split a long document into smaller chunks of a specified length\n",
        "chunks = chunk_text(text=pure_text, max_length=512)\n",
        "\n",
        "# You can see the number of words and contents in each paragraph.\n",
        "print(\"Review the results of splitting the long text into several short texts.\\n\")\n",
        "for index, chunk in enumerate(chunks):\n",
        "    if index == 0:\n",
        "        print(f\"\\n========== The {index + 1}-st segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "    elif index == 1:\n",
        "        print(f\"\\n========== The {index + 1}-nd segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "    elif index == 2:\n",
        "        print(f\"\\n========== The {index + 1}-rd segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "    else:\n",
        "        print(f\"\\n========== The {index + 1}-th segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuvx30fkW4kU"
      },
      "source": [
        "# Part4 - Summarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mr9Kz634zT2"
      },
      "source": [
        "## **You only need to choose one of the following parts.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCPYjOAyXWaE"
      },
      "source": [
        "## **If you want to use ChatGPT, begin with this part.**\n",
        "##### (1) You can refer to https://shorturl.at/X0NDY (Page 44) for obtaining ChatGPT API key.\n",
        "##### (2) You can refer to https://platform.openai.com/docs/models/overview for more details about models you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YCara20SW8AN"
      },
      "outputs": [],
      "source": [
        "def summarization(client, summarization_prompt, model_name=\"gpt-3.5-turbo\", temperature=0.0, top_p=1.0, max_tokens=512):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - Use the OpenAI Chat API to summarize a given text.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - client: OpenAI Chat API client.\n",
        "        - summarization_prompt: The summarization prompt including the text which need to be summarized.\n",
        "        - model_name: The model name, default is \"gpt-3.5-turbo\". You can refer to \"https://platform.openai.com/docs/models/overview\" for more details.\n",
        "        - temperature: Controls randomness in the response. Lower values make responses more deterministic, default is 0.0.\n",
        "        - top_p: Controls diversity via nucleus sampling. Higher values lead to more diverse responses, default is 1.0.\n",
        "        - max_tokens: The maximum number of tokens to generate in the completion, default is 512.\n",
        "\n",
        "    (3) Return:\n",
        "        - The summarized text.\n",
        "\n",
        "    (4) Example:\n",
        "        - If the text is \"ABC\" and the summarization prompt is \"DEF\", system_prompt is \"GHI\", model_name is \"gpt-3.5-turbo\",\n",
        "          temperature is 0.0, top_p is 1.0, and max_tokens is 512, then you can call the function like this:\n",
        "\n",
        "              summarization(client=client, text=\"ABC\", summarization_prompt=\"DEF\", system_prompt=\"GHI\", model_name=\"gpt-3.5-turbo\", temperature=0.0, top_p=1.0, max_tokens=512)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # The user prompt is a concatenation of the summarization_prompt and text.\n",
        "    user_prompt = summarization_prompt\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            # Use the OpenAI Chat API to summarize the text.\n",
        "            chat_completion = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": user_prompt,\n",
        "                    }\n",
        "                ],\n",
        "                    model=model_name,\n",
        "                    temperature=temperature,\n",
        "                    top_p=top_p,\n",
        "                    max_tokens=max_tokens\n",
        "            )\n",
        "\n",
        "            break\n",
        "\n",
        "        except:\n",
        "            # If the API call fails, wait for 1 second and try again.\n",
        "            print(\"The API call fails, wait for 1 second and try again.\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "p3VZeUfBYcih",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of ChatGPT { run: \"auto\" }\n",
        "''' ===== In this block, you can modify your desired parameters and set your OpenAI API key ===== '''\n",
        "\n",
        "# Your OpenAI API key.\n",
        "# @markdown **openai_api_key**: Your OpenAI API key.\n",
        "openai_api_key = \"YOUR_GHATGPT_API_KEY\" # @param {type:\"string\"}\n",
        "\n",
        "# The model name, default is \"gpt-3.5-turbo\". You can refer to \"https://platform.openai.com/docs/models/overview\" for more details.\n",
        "# @markdown **model_name**: The model name, default is \"gpt-3.5-turbo\". You can refer to \"https://platform.openai.com/docs/models/overview\" for more details.\n",
        "model_name = \"gpt-3.5-turbo\" # @param {type: \"string\"}\n",
        "\n",
        "# Controls randomness in the response. Lower values make responses more deterministic.\n",
        "# @markdown **temperature**: Controls randomness in the response. Lower values make responses more deterministic.\n",
        "temperature = 0 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "# @markdown **top_p**: Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "top_p = 0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ysnuD_eIeWjY"
      },
      "outputs": [],
      "source": [
        "# Construct openai client.\n",
        "client = OpenAI(api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwDxai-YY4Hl"
      },
      "source": [
        "The code block below takes about **30** seconds to run when using the **gpt-3.5-turbo** model, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvCBfcW7Qd-e"
      },
      "source": [
        "### We offer the following two methods for summarization.\n",
        "Reference: https://reurl.cc/VzagLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9QC8lG_QRZL"
      },
      "source": [
        "#### **If you want to use the method of Multi-Stage Summarization, begin with this part.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fUma_oXAQtmg"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of ChatGPT Multi-Stage Summarization: Paragraph { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# The maximum number of tokens to generate in the completion.\n",
        "# @markdown **max_tokens**: The maximum number of tokens to generate in the completion.\n",
        "max_tokens = 350 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"用 300 個字內寫出這段文字的摘要，其中包括要點和所有重要細節：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQtk04XATy1j"
      },
      "source": [
        "##### Step1: Split the long text into multiple smaller pieces and obtain summaries for each smaller text piece separately"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUTFgIGl1IRp"
      },
      "source": [
        "The code block below takes about **80** seconds to run when using the (1) **gpt-3.5-turbo** model, (2) length of chunks is 512 and (3) maximum number of tokens is 250, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Nv-Ko3UZYjpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83007bff-3886-4abc-e1fd-969c5ef7f10d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------Summary of Segment 1----------------------------\n",
            "\n",
            "學問是需要透過實際做來獲得的，單純聽或讀可能無法真正吸收知識。舉例來說，修課最後的final project能幫助整合所學，讓知識真正被掌握。另外，寫程式也能幫\n",
            "\n",
            "助理解複雜演算法，透過實際操作才能真正理解。因此，我們應該記得多動手多做，只有在實際操作中，學問才能成為自己的。成績單上的數字並不能完全反映學問的深度。\n",
            "\n",
            "Length of summary for segment 1: 156\n",
            "Time taken to generate summary for segment 1: 3.45 sec.\n",
            "\n",
            "----------------------------Summary of Segment 2----------------------------\n",
            "\n",
            "許多人認為某些課程不重要，甚至選擇不修讀因為覺得太辛苦或無法獲得具體成績。然而，這些課程可能提供最多學習機會，對全面學習有幫助。重要的是不要錯過這些機會，培養自\n",
            "\n",
            "己的思考能力和習慣。透過閱讀數學式子和思考課程內容，可以深入理解並提升思考能力。這是最重要的學習方法，需要花費大量時間和精力。\n",
            "\n",
            "Length of summary for segment 2: 143\n",
            "Time taken to generate summary for segment 2: 3.4 sec.\n",
            "\n",
            "----------------------------Summary of Segment 3----------------------------\n",
            "\n",
            "學習不僅僅是在課業內，還包括課業外的各種活動。學習是一種增長、進步和獲得快樂的過程。例如，打球可以提升健康、手腦協調、團隊精神和人際互動能力，是一個很好的學習機\n",
            "\n",
            "會。爬山和旅行也是很好的學習機會，可以增長見識、擴展視野，並帶來快樂。無論是什麼活動，只要讓你感到增長、進步和快樂，都值得花時間和精力去學習。\n",
            "\n",
            "Length of summary for segment 3: 151\n",
            "Time taken to generate summary for segment 3: 4.05 sec.\n",
            "\n",
            "----------------------------Summary of Segment 4----------------------------\n",
            "\n",
            "談戀愛和交朋友都是學習人際互動和溝通的好機會，即使沒有緣分也可以透過交朋友來學習。在大學裡，有很多機會參加各種活動，例如戲劇社或舞蹈社，這些活動可以幫助我們成長\n",
            "\n",
            "和進步。即使不是表演者，參與幕後工作也能獲得寶貴的經驗。因此，大家應該積極參與各種活動，透過互動和合作來提升自己。\n",
            "\n",
            "Length of summary for segment 4: 137\n",
            "Time taken to generate summary for segment 4: 3.48 sec.\n",
            "\n",
            "----------------------------Summary of Segment 5----------------------------\n",
            "\n",
            "這段文字談論到在學習過程中，除了專業知識外，參與各種活動和團隊合作也是非常重要的。這些活動可以幫助人們成長和進步，並提供寶貴的學習機會。即使這些活動沒有考試或成\n",
            "\n",
            "績，但它們對個人發展和職業目標都有著重要的影響。在電機工程領域，成功很少是一個人能夠完成的，需要與他人合作。因此，學習如何進入團隊、成為領導者以及推動想法的能力\n",
            "\n",
            "都是非常重要的。這些被稱為軟實力，是除了專業技能外的另一種重要能力。\n",
            "\n",
            "Length of summary for segment 5: 194\n",
            "Time taken to generate summary for segment 5: 4.1 sec.\n",
            "\n",
            "----------------------------Summary of Segment 6----------------------------\n",
            "\n",
            "成功與否不僅取決於硬實力，更重要的是軟實力，包括人際交往能力、溝通能力、領導能力等。這些soft skills對於成功至關重要，並且可以透過課外活動和學習機會來\n",
            "\n",
            "培養。雖然有些人天生具備這些能力，但大多數人需要努力培養。作者分享了自己在大學期間如何透過努力培養soft\n",
            "\n",
            "skills，並強調這些能力對於電機工程師的成功至關重要。\n",
            "\n",
            "Length of summary for segment 6: 163\n",
            "Time taken to generate summary for segment 6: 3.49 sec.\n",
            "\n",
            "----------------------------Summary of Segment 7----------------------------\n",
            "\n",
            "在這段文字中，作者談到了一生career的發展，認為黃金時期是在35歲到55歲之間。他指出這20年是最好的，因為在這之前可能各方面尚未具備，而在之後年紀大了會有\n",
            "\n",
            "一些折扣。作者觀察到電機系的畢業同學的發展，有些人斜率慢慢saturate，有些人斜率高但會overshoot後收斂，也有些人一直向上走沒有saturate。他\n",
            "\n",
            "認為影響發展的主要因素是實力、努力、大智和self skill，而不是單一學科的成績。最後，作者強調這四個因素對發展的影響。\n",
            "\n",
            "Length of summary for segment 7: 222\n",
            "Time taken to generate summary for segment 7: 4.57 sec.\n",
            "\n",
            "----------------------------Summary of Segment 8----------------------------\n",
            "\n",
            "這段文字主要談到了實力、努力、大智和self skills這四個要素對於個人成長的重要性。實力是指在電機工程領域中各種技能的累積，要透過全面的學習來提升自己的實\n",
            "\n",
            "力，避免過度擬合。努力是每個人都應該具備的品質，不斷努力可以使個人在未來有所成就。self\n",
            "\n",
            "skills則是指平時被忽視但卻重要的技能，通過在課業之外的事情上努力提升自己。最後，每個人都應該設定自己的長程目標，這將有助於個人成長和發展。\n",
            "\n",
            "Length of summary for segment 8: 198\n",
            "Time taken to generate summary for segment 8: 4.21 sec.\n",
            "\n",
            "----------------------------Summary of Segment 9----------------------------\n",
            "\n",
            "我希望在未來的5年、10年、15年甚至更長的時間裡，將我的努力集中在實現一些非常重要的事情上。這些事情是我非常想要做的，也是我認為具有意義的。這就是我的長程目標\n",
            "\n",
            "。對於那些能夠想清楚自己的長程目標的人來說，他們更容易朝著目標前進。我認為這四件事情是真正影響我未來的關鍵。\n",
            "\n",
            "Length of summary for segment 9: 134\n",
            "Time taken to generate summary for segment 9: 3.17 sec.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    # Record the start time.\n",
        "    start = time.time()\n",
        "\n",
        "    # Construct summarization prompt.\n",
        "    summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "    # We summarize each section that has been split up separately.\n",
        "    response = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "    # Calculate the execution time and round it to 2 decimal places.\n",
        "    cost_time = round(time.time() - start, 2)\n",
        "\n",
        "    # Print the summary and its length.\n",
        "    print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "    for text in textwrap.wrap(response, 80):\n",
        "        print(f\"{text}\\n\")\n",
        "    print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "    print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Record the result.\n",
        "    paragraph_summarizations.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "w6QXhSK_eACs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8657f8f0-de4c-485a-a0f6-d56763e73deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of segment 1: 學問是需要透過實際做來獲得的，單純聽或讀可能無法真正吸收知識。舉例來說，修課最後的final project能幫助整合所學，讓知識真正被掌握。另外，寫程式也能幫助理解複雜演算法，透過實際操作才能真正理解。因此，我們應該記得多動手多做，只有在實際操作中，學問才能成為自己的。成績單上的數字並不能完全反映學問的深度。\n",
            "Summary of segment 2: 許多人認為某些課程不重要，甚至選擇不修讀因為覺得太辛苦或無法獲得具體成績。然而，這些課程可能提供最多學習機會，對全面學習有幫助。重要的是不要錯過這些機會，培養自己的思考能力和習慣。透過閱讀數學式子和思考課程內容，可以深入理解並提升思考能力。這是最重要的學習方法，需要花費大量時間和精力。\n",
            "Summary of segment 3: 學習不僅僅是在課業內，還包括課業外的各種活動。學習是一種增長、進步和獲得快樂的過程。例如，打球可以提升健康、手腦協調、團隊精神和人際互動能力，是一個很好的學習機會。爬山和旅行也是很好的學習機會，可以增長見識、擴展視野，並帶來快樂。無論是什麼活動，只要讓你感到增長、進步和快樂，都值得花時間和精力去學習。\n",
            "Summary of segment 4: 談戀愛和交朋友都是學習人際互動和溝通的好機會，即使沒有緣分也可以透過交朋友來學習。在大學裡，有很多機會參加各種活動，例如戲劇社或舞蹈社，這些活動可以幫助我們成長和進步。即使不是表演者，參與幕後工作也能獲得寶貴的經驗。因此，大家應該積極參與各種活動，透過互動和合作來提升自己。\n",
            "Summary of segment 5: 這段文字談論到在學習過程中，除了專業知識外，參與各種活動和團隊合作也是非常重要的。這些活動可以幫助人們成長和進步，並提供寶貴的學習機會。即使這些活動沒有考試或成績，但它們對個人發展和職業目標都有著重要的影響。在電機工程領域，成功很少是一個人能夠完成的，需要與他人合作。因此，學習如何進入團隊、成為領導者以及推動想法的能力都是非常重要的。這些被稱為軟實力，是除了專業技能外的另一種重要能力。\n",
            "Summary of segment 6: 成功與否不僅取決於硬實力，更重要的是軟實力，包括人際交往能力、溝通能力、領導能力等。這些soft skills對於成功至關重要，並且可以透過課外活動和學習機會來培養。雖然有些人天生具備這些能力，但大多數人需要努力培養。作者分享了自己在大學期間如何透過努力培養soft skills，並強調這些能力對於電機工程師的成功至關重要。\n",
            "Summary of segment 7: 在這段文字中，作者談到了一生career的發展，認為黃金時期是在35歲到55歲之間。他指出這20年是最好的，因為在這之前可能各方面尚未具備，而在之後年紀大了會有一些折扣。作者觀察到電機系的畢業同學的發展，有些人斜率慢慢saturate，有些人斜率高但會overshoot後收斂，也有些人一直向上走沒有saturate。他認為影響發展的主要因素是實力、努力、大智和self skill，而不是單一學科的成績。最後，作者強調這四個因素對發展的影響。\n",
            "Summary of segment 8: 這段文字主要談到了實力、努力、大智和self skills這四個要素對於個人成長的重要性。實力是指在電機工程領域中各種技能的累積，要透過全面的學習來提升自己的實力，避免過度擬合。努力是每個人都應該具備的品質，不斷努力可以使個人在未來有所成就。self skills則是指平時被忽視但卻重要的技能，通過在課業之外的事情上努力提升自己。最後，每個人都應該設定自己的長程目標，這將有助於個人成長和發展。\n",
            "Summary of segment 9: 我希望在未來的5年、10年、15年甚至更長的時間裡，將我的努力集中在實現一些非常重要的事情上。這些事情是我非常想要做的，也是我認為具有意義的。這就是我的長程目標。對於那些能夠想清楚自己的長程目標的人來說，他們更容易朝著目標前進。我認為這四件事情是真正影響我未來的關鍵。\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# First, we collect all the summarizations obtained before and print them.\n",
        "\n",
        "collected_summarization = \"\"\n",
        "for index, paragraph_summarization in enumerate(paragraph_summarizations):\n",
        "    collected_summarization += f\"Summary of segment {index + 1}: {paragraph_summarization}\\n\"\n",
        "\n",
        "print(collected_summarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itdqp3H5T2T7"
      },
      "source": [
        "##### Step2: After obtaining summaries for each smaller text piece separately, process these summaries to generate the final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "l0ghZaKfVEyN"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of ChatGPT Multi-Stage Summarization: Total { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"使用繁體中文在 500 字以內寫出以下文字的簡潔摘要：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka0JcOvYVIWu"
      },
      "source": [
        "The code block below takes about **10** seconds to run when using the (1) **gpt-3.5-turbo** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4zHPMRDWeCuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05a3c7f-a2f5-466f-ec57-83236e6bceff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------Final Summary----------------------------\n",
            "\n",
            "總結第一部分：學問需要透過實際做來獲得，單純聽或讀可能無法真正吸收知識。修課最後的final\n",
            "project和寫程式能幫助整合所學，讓知識真正被掌握。成績單上的數字並不能完全反映學問的深度。  總結第二部分：某些課程可能提供最多學習機會，對全面學習有幫助\n",
            "。重要的是不要錯過這些機會，培養自己的思考能力和習慣。透過閱讀數學式子和思考課程內容，可以深入理解並提升思考能力。\n",
            "總結第三部分：學習不僅僅是在課業內，還包括課業外的各種活動。打球、爬山和旅行等活動都是很好的學習機會，可以增長見識、擴展視野，並帶來快樂。\n",
            "總結第四部分：談戀愛和交朋友都是學習人際互動和溝通的好機會，參與各種活動可以幫助我們成長和進步。大家應該積極參與各種活動，透過互動和合作來提升自己。\n",
            "總結第五部分：參與各種活動和團隊合作是非常重要的，即使沒有考試或成績，這些活動對個人發展和職業目標都有著重要的影響。學\n",
            "\n",
            "Length of final summary: 392\n",
            "Time taken to generate the final summary: 6.94 sec.\n"
          ]
        }
      ],
      "source": [
        "# Finally, we compile a final summary from the summaries of each section.\n",
        "\n",
        "# Record the start time.\n",
        "start = time.time()\n",
        "\n",
        "# Run final summarization.\n",
        "summarization_prompt = summarization_prompt_template.replace(\"<text>\", collected_summarization)\n",
        "final_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "# Calculate the execution time and round it to 2 decimal places.\n",
        "cost_time = round(time.time() - start, 2)\n",
        "\n",
        "# Print the summary and its length.\n",
        "print(f\"----------------------------Final Summary----------------------------\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 80):\n",
        "        print(f\"{text}\")\n",
        "print(f\"\\nLength of final summary: {len(final_summarization)}\")\n",
        "print(f\"Time taken to generate the final summary: {cost_time} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Qi7eERB8eGdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8965cb-27b0-4c05-cd77-38ffac065d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final summary has been saved to ./final-summary-信號與人生-chatgpt-multi-stage.txt\n",
            "\n",
            "===== Below is the final summary (392 words) =====\n",
            "\n",
            "總結第一部分：學問需要透過實際做來獲得，單純聽或讀可能無法真正吸收知識。修課最後的final\n",
            "project和寫程式能幫助整合所學，讓知識真正被掌握。成績單上的數字並不能完全反映學問的深度。  總結第二部分：某些課程可能提\n",
            "供最多學習機會，對全面學習有幫助。重要的是不要錯過這些機會，培養自己的思考能力和習慣。透過閱讀數學式子和思考課程內容，可以深入理\n",
            "解並提升思考能力。  總結第三部分：學習不僅僅是在課業內，還包括課業外的各種活動。打球、爬山和旅行等活動都是很好的學習機會，可以\n",
            "增長見識、擴展視野，並帶來快樂。  總結第四部分：談戀愛和交朋友都是學習人際互動和溝通的好機會，參與各種活動可以幫助我們成長和進\n",
            "步。大家應該積極參與各種活動，透過互動和合作來提升自己。\n",
            "總結第五部分：參與各種活動和團隊合作是非常重要的，即使沒有考試或成績，這些活動對個人發展和職業目標都有著重要的影響。學\n"
          ]
        }
      ],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-chatgpt-multi-stage.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    final_summarization = cc.convert(final_summarization)\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(final_summarization)\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(final_summarization)} words) =====\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 64):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRzf_0cTV6TS"
      },
      "source": [
        "#### **If you want to use the method of Refinement, begin with this part.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "k79C13kWW_Ye"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of ChatGPT Refinement { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template** and **summarization_prompt_refine_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "\n",
        "# Initial prompt.\n",
        "# @markdown **summarization_prompt_template**: Initial prompt.\n",
        "summarization_prompt_template = \"請在 300 字以內，提供以下文字的簡潔摘要:<text>\" # @param {type:\"string\"}\n",
        "\n",
        "# Refinement prompt.\n",
        "# @markdown **summarization_prompt_refinement_template**: Refinement prompt.\n",
        "summarization_prompt_refinement_template = \"請在 500 字以內，結合原先的摘要和新的內容，提供簡潔的摘要:<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEtfC9WeZkNH"
      },
      "source": [
        "The code block below takes about **200** seconds to run when using the (1) **gpt-3.5-turbo** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rugUlJ6HeZPF"
      },
      "source": [
        "Pipeline of the method of Refinement.\n",
        "\n",
        "Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "\n",
        "Step2: For each following document, the previous output is fed in along with the new document.\n",
        "\n",
        "Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "\n",
        "Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "8aFmk1ATAOdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7c09c8-3414-4af1-965a-a26ed440c8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------Summary of Segment 1----------------------------\n",
            "\n",
            "我希望在未來的5年、10年、15年甚至更長的時間裡，將我的努力集中在實現一些非常重要的事情上。這些事情是我非常想要做的，也是我認為具有意義的。這就是我的長程目標\n",
            "\n",
            "。對於那些能夠想清楚自己的長程目標的人來說，他們更容易朝著目標前進。我認為這四件事情是真正影響我未來的關鍵。\n",
            "\n",
            "Length of summary for segment 1: 134\n",
            "Time taken to generate summary for segment 1: 2.87 sec.\n",
            "\n",
            "----------------------------Summary of the First 2 Segments----------------------------\n",
            "\n",
            "學問是做出來的，透過實際動手做final project或寫程式等方式，將所學知識應用整合，才能真正理解和掌握。儘管有些人可能覺得這些做法太辛苦，並且可能不會立\n",
            "\n",
            "即反映在成績上，但這些實作方式卻是讓你學到最多、全面學習的機會。在學習的過程中，不要漏掉這些重要的事情，培養自己的思考能力和習慣。透過思考數學式子和閱讀的同時，\n",
            "\n",
            "深入理解知識背後的意義，這才是最重要的。因此，記得多動手多做，讓學問變成自己的。成績不一定代表真正的學習成果，重要的是你如何運用所學來提升自己的能力和思考水平。\n",
            "\n",
            "Length of summary for the first 2 segments: 240\n",
            "Time taken to generate summary for the first 2 segments: 5.44 sec.\n",
            "\n",
            "----------------------------Summary of the First 3 Segments----------------------------\n",
            "\n",
            "學問是透過實際動手做final project或寫程式等方式，將所學知識應用整合，才能真正理解和掌握。儘管有些人可能覺得這些做法太辛苦，並且可能不會立即反映在成\n",
            "\n",
            "績上，但這些實作方式卻是讓你學到最多、全面學習的機會。在學習的過程中，不要漏掉這些重要的事情，培養自己的思考能力和習慣。透過思考數學式子和閱讀的同時，深入理解知\n",
            "\n",
            "識背後的意義，這才是最重要的。因此，記得多動手多做，讓學問變成自己的。成績不一定代表真正的學習成果，重要的是你如何運用所學來提升自己的能力和思考水平。  除了課\n",
            "\n",
            "業內的學習，課業外也有很多值得學習的地方。學習不僅是在課堂上，而是一種增長、進步和獲得快樂的過程。例如，打球可以提升健康、手腦協調、團隊精神和人際互動能力，是一\n",
            "\n",
            "個有增長的學習機會。爬山可以讓人學到很多，旅行則可以擴展視野、增加見識，都是值得下功夫去做的學習機會。不論是什麼活動，只要讓你有增長、進步和快樂，都是值得投入時\n",
            "\n",
            "間和精力的學習之\n",
            "\n",
            "Length of summary for the first 3 segments: 408\n",
            "Time taken to generate summary for the first 3 segments: 8.88 sec.\n",
            "\n",
            "----------------------------Summary of the First 4 Segments----------------------------\n",
            "\n",
            "學習不僅限於課堂內的知識，還包括課外的各種活動和經驗。打球、爬山、旅行等都是有增長和進步的學習機會。此外，談戀愛和交朋友也是一種學習，讓人體驗人與人之間的感覺和\n",
            "\n",
            "期待，提升人際互動和溝通能力。在校園裡，參加各種活動如舞蹈、劇場等也能帶來成長和進步。無論是參與活動還是交朋友，都是值得投入時間和精力的學習之旅。\n",
            "\n",
            "Length of summary for the first 4 segments: 153\n",
            "Time taken to generate summary for the first 4 segments: 3.85 sec.\n",
            "\n",
            "----------------------------Summary of the First 5 Segments----------------------------\n",
            "\n",
            "學習不僅僅是在課堂上獲取知識，還包括參與各種課外活動和經歷。打球、爬山、旅行等都是讓人成長和進步的機會，同時談戀愛和交朋友也是一種學習，提升人際互動和溝通能力。\n",
            "\n",
            "在校園裡參加各種活動如舞蹈、劇場等也能帶來成長和進步。此外，參與服裝道具組、戲劇學會等校內外活動也是很有價值的學習機會。這些活動雖然沒有考試成績，但對於個人發展\n",
            "\n",
            "和團隊合作能力都非常重要。在現今的電機工程領域，個人很難獨自完成任務，必須與他人合作。因此，學習如何進入團隊、成為領導者並推動事情的發展是非常重要的。這些被稱為\n",
            "\n",
            "軟實力的技能，是除了專業知識外同樣重要的能力。\n",
            "\n",
            "Length of summary for the first 5 segments: 263\n",
            "Time taken to generate summary for the first 5 segments: 7.35 sec.\n",
            "\n",
            "----------------------------Summary of the First 6 Segments----------------------------\n",
            "\n",
            "在學習的過程中，除了在課堂上獲取知識外，參與各種課外活動和經歷也是非常重要的。打球、爬山、旅行等活動都能讓人成長和進步，同時談戀愛和交朋友也是一種學習，有助於提\n",
            "\n",
            "升人際互動和溝通能力。在校園裡參加各種活動如舞蹈、劇場等也能帶來成長和進步。參與服裝道具組、戲劇學會等校內外活動也是很有價值的學習機會。這些活動雖然沒有考試成績\n",
            "\n",
            "，但對個人發展和團隊合作能力都非常重要。  在現今的電機工程領域，個人很難獨自完成任務，必須與他人合作。因此，學習如何進入團隊、成為領導者並推動事情的發展是非常\n",
            "\n",
            "重要的。這些被稱為軟實力的技能，是除了專業知識外同樣重要的能力。在這個城市能力這種是硬實力，軟實力主要是各種人際之間的能力，包括溝通能力、協調能力、交朋友的能力\n",
            "\n",
            "、說服人的能力、團隊精神和領導能力等等。成功的電機工程師通常擁有豐富的軟實力，這是他們成功的關鍵之一。  這些軟實力的能力並非天生，大部分人需要努\n",
            "\n",
            "Length of summary for the first 6 segments: 393\n",
            "Time taken to generate summary for the first 6 segments: 7.55 sec.\n",
            "\n",
            "----------------------------Summary of the First 7 Segments----------------------------\n",
            "\n",
            "在學習的過程中，參與各種課外活動和經歷是非常重要的，這些活動能讓人成長和進步，同時提升人際互動和溝通能力。在現今的電機工程領域，個人很難獨自完成任務，必須與他人\n",
            "\n",
            "合作，因此軟實力的技能如溝通能力、協調能力、交朋友的能力、團隊精神和領導能力等同樣重要。成功的電機工程師通常擁有豐富的軟實力，這是他們成功的關鍵之一。\n",
            "\n",
            "在一生career的發展中，35歲到55歲被視為黃金時代，這段時間是最好的發展時期。不同人的發展軌跡可能有所不同，但實力、努力、大智和self skill是影響\n",
            "\n",
            "發展的主要因素。將每門必修課視為單一跑道並不足以衡量成功，真正重要的是這些因素如何影響個人的整體發展。因此，除了專業知識外，培養軟實力和全面發展自己的能力對於電\n",
            "\n",
            "機工程師的職業發展至關重要。\n",
            "\n",
            "Length of summary for the first 7 segments: 331\n",
            "Time taken to generate summary for the first 7 segments: 7.42 sec.\n",
            "\n",
            "----------------------------Summary of the First 8 Segments----------------------------\n",
            "\n",
            "在電機工程領域的職業發展中，35歲到55歲被視為黃金時代，這段時間是最好的發展時期。不同人的發展軌跡可能有所不同，但實力、努力、大智和自我技能是影響發展的主要因\n",
            "\n",
            "素。成功的電機工程師通常擁有豐富的軟實力，這是他們成功的關鍵之一。在學習的過程中，參與各種課外活動和經歷是非常重要的，這些活動能讓人成長和進步，同時提升人際互動\n",
            "\n",
            "和溝通能力。除了專業知識外，培養軟實力和全面發展自己的能力對於電機工程師的職業發展至關重要。實力、努力、大智和自我技能是影響電機工程師職業發展的主要因素，而全面\n",
            "\n",
            "的學習和參與各種課外活動能夠提升個人的軟實力，進而影響整體發展。建立長程目標也是重要的，每個人都可以思考自己想要達成的目標，並在適當的時機開始努力實現。這些因素\n",
            "\n",
            "共同影響著電機工程師在職業生涯中的成功與成長。\n",
            "\n",
            "Length of summary for the first 8 segments: 343\n",
            "Time taken to generate summary for the first 8 segments: 6.72 sec.\n",
            "\n",
            "----------------------------Summary of the First 9 Segments----------------------------\n",
            "\n",
            "在電機工程領域的職業發展中，35歲到55歲被視為黃金時代，這段時間是最好的發展時期。不同人的發展軌跡可能有所不同，但實力、努力、大智和自我技能是影響發展的主要因\n",
            "\n",
            "素。成功的電機工程師通常擁有豐富的軟實力，這是他們成功的關鍵之一。在學習的過程中，參與各種課外活動和經歷是非常重要的，這些活動能讓人成長和進步，同時提升人際互動\n",
            "\n",
            "和溝通能力。除了專業知識外，培養軟實力和全面發展自己的能力對於電機工程師的職業發展至關重要。實力、努力、大智和自我技能是影響電機工程師職業發展的主要因素，而全面\n",
            "\n",
            "的學習和參與各種課外活動能夠提升個人的軟實力，進而影響整體發展。建立長程目標也是重要的，每個人都可以思考自己想要達成的目標，並在適當的時機開始努力實現。這些因素\n",
            "\n",
            "共同影響著電機工程師在職業生涯中的成功與成長。  此外，設定長程目標對於個人的職業發展也是至關重要的。無論是花費5年、10年、15年甚至更長的時間，將努力\n",
            "\n",
            "Length of summary for the first 9 segments: 397\n",
            "Time taken to generate summary for the first 9 segments: 8.14 sec.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    if index == 0:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Construct summarization prompt.\n",
        "        summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "        # Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "        first_paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(first_paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # Print the summary and its length.\n",
        "        print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "        for text in textwrap.wrap(response, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "        print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Step2: For each following document, the previous output is fed in along with the new document.\n",
        "        chunk_text = f\"\"\"前 {index} 段的摘要: {paragraph_summarizations[-1]}\\n第 {index + 1} 段的內容: {chunk}\"\"\"\n",
        "\n",
        "        # Construct refinement prompt for summarization.\n",
        "        summarization_prompt = summarization_prompt_refinement_template.replace(\"<text>\", chunk_text)\n",
        "\n",
        "        # Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "        paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # print results.\n",
        "        print(f\"----------------------------Summary of the First {index + 1} Segments----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for the first {index + 1} segments: {len(paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for the first {index + 1} segments: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "EaXWOLIOa4dM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09df148c-3cce-4f61-8f4d-414d7d5ebd69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final summary has been saved to ./final-summary-信號與人生-chatgpt-refinement.txt\n",
            "\n",
            "===== Below is the final summary (397 words) =====\n",
            "\n",
            "在電機工程領域的職業發展中，35歲到55歲被視為黃金時代，這段時間是最好的發展時期。不同人的發展軌跡可能有所不同，但實力、努力、大智和自我技能是影響發展的主要因\n",
            "素。成功的電機工程師通常擁有豐富的軟實力，這是他們成功的關鍵之一。在學習的過程中，參與各種課外活動和經歷是非常重要的，這些活動能讓人成長和進步，同時提升人際互動\n",
            "和溝通能力。除了專業知識外，培養軟實力和全面發展自己的能力對於電機工程師的職業發展至關重要。實力、努力、大智和自我技能是影響電機工程師職業發展的主要因素，而全面\n",
            "的學習和參與各種課外活動能夠提升個人的軟實力，進而影響整體發展。建立長程目標也是重要的，每個人都可以思考自己想要達成的目標，並在適當的時機開始努力實現。這些因素\n",
            "共同影響著電機工程師在職業生涯中的成功與成長。  此外，設定長程目標對於個人的職業發展也是至關重要的。無論是花費5年、10年、15年甚至更長的時間，將努力\n"
          ]
        }
      ],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-chatgpt-refinement.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    paragraph_summarizations[-1] = cc.convert(paragraph_summarizations[-1])\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(paragraph_summarizations[-1])\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(paragraph_summarizations[-1])} words) =====\\n\")\n",
        "for text in textwrap.wrap(paragraph_summarizations[-1], 80):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCM4kPuBXh7R"
      },
      "source": [
        "## **If you want to use Gemini, begin with this part.**\n",
        "##### (1) You can refer to https://shorturl.at/X0NDY (Page 35) for obtaining Gemini API key.\n",
        "##### (2) You can refer to https://ai.google.dev/models/gemini for more details about which models you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anuFrhHNkMgY"
      },
      "outputs": [],
      "source": [
        "def summarization(summarization_prompt, model_name=\"gemini-pro\", temperature=0.0, top_p=1.0, max_tokens=512):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - Use the OpenAI Chat API to summarize a given text.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - summarization_prompt: The summarization prompt.\n",
        "        - model_name: The model name, default is \"gemini-pro\". You can refer to \"https://ai.google.dev/models/gemini\" for more details.\n",
        "        - temperature: Controls randomness in the response. Lower values make responses more deterministic, default is 0.0.\n",
        "        - top_p: Controls diversity via nucleus sampling. Higher values lead to more diverse responses, default is 1.0.\n",
        "        - max_tokens: The maximum number of tokens to generate in the completion, default is 512.\n",
        "\n",
        "    (3) Return:\n",
        "        - The summarized text.\n",
        "\n",
        "    (4) Example:\n",
        "        - If the text is \"ABC\" and the summarization prompt is \"DEF\", model_name is \"gemini-pro\",\n",
        "          temperature is 0.0, top_p is 1.0, and max_tokens is 512, then you can call the function like this:\n",
        "\n",
        "              summarization(text=\"ABC\", summarization_prompt=\"DEF\", model_name=\"gemini-pro\", temperature=0.0, top_p=1.0, max_tokens=512)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # The user prompt is a concatenation of the summarization_prompt and text.\n",
        "    user_prompt = summarization_prompt\n",
        "\n",
        "    # Load the generative model.\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "\n",
        "    # Set the generation configuration.\n",
        "    generation_config = genai.GenerationConfig(temperature=temperature, top_p=top_p, max_output_tokens=max_tokens)\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            # Use the OpenAI Chat API to summarize the text.\n",
        "            response = model.generate_content(contents=user_prompt, generation_config=generation_config)\n",
        "\n",
        "            break\n",
        "\n",
        "        except:\n",
        "            # If the API call fails, wait for 1 second and try again.\n",
        "            print(\"The API call fails, wait for 1 second and try again.\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plyvWCvXllz3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of Gemini { run: \"auto\" }\n",
        "''' In this block, you can modify your desired parameters and set your api key. '''\n",
        "\n",
        "# Your google api key.\n",
        "# @markdown **google_api_key**: Your google api key.\n",
        "google_api_key = \"YOUR_GOOGLE_API_KEY\" # @param {type:\"string\"}\n",
        "\n",
        "# The model name. You can refer to \"https://ai.google.dev/models/gemini\" for more details.\n",
        "# @markdown **model_name**: The model name. You can refer to \"https://ai.google.dev/models/gemini\" for more details.\n",
        "model_name = \"gemini-pro\" # @param {type:\"string\"}\n",
        "\n",
        "# Controls randomness in the response. Lower values make responses more deterministic\n",
        "# @markdown **temperature**: Controls randomness in the response. Lower values make responses more deterministic.\n",
        "temperature = 0.0 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Controls diversity via nucleus sampling. Higher values lead to more diverse responses\n",
        "# @markdown **top_p**: Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "top_p = 1.0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eed0NjqJtGfM"
      },
      "outputs": [],
      "source": [
        "# Set Google API key.\n",
        "genai.configure(api_key=google_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csw7hxrHsJym"
      },
      "source": [
        "### We offer the following two methods for summarization.\n",
        "Reference: https://reurl.cc/VzagLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGHFGCyasp9h"
      },
      "source": [
        "#### **If you want to use the method of Multi-Stage Summarization, begin with this part.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TsX6dBgs1iw"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Multi-Stage Summarization: Paragraph { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# The maximum number of tokens to generate in the completion.\n",
        "# @markdown **max_tokens**: The maximum number of tokens to generate in the completion.\n",
        "max_tokens = 350 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"用 300 個字內寫出這段文字的摘要，其中包括要點和所有重要細節：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkQdOOixuJU9"
      },
      "source": [
        "##### Step1: Split the long text into multiple smaller pieces and obtain summaries for each smaller text piece separately\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olrUkpe615-E"
      },
      "source": [
        "The code block below takes about **40** seconds to run when using the (1) **gemini-pro** model, (2) length of chunks is 512 and (3) maximum number of tokens is 350, but the actual time may vary depending on the condition of Colab and the status of the Google API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KOp_d6XloCn"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    # Record the start time.\n",
        "    start = time.time()\n",
        "\n",
        "    # Construct summarization prompt.\n",
        "    summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "    # We summarize each section that has been split up separately.\n",
        "    response = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "    # Calculate the execution time and round it to 2 decimal places.\n",
        "    cost_time = round(time.time() - start, 2)\n",
        "\n",
        "    # Print the summary and its length.\n",
        "    print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "    for text in textwrap.wrap(response, 80):\n",
        "        print(f\"{text}\\n\")\n",
        "    print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "    print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Record the result.\n",
        "    paragraph_summarizations.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amp-gOibmpvK"
      },
      "outputs": [],
      "source": [
        "# First, we collect all the summarizations obtained before and print them.\n",
        "\n",
        "collected_summarization = \"\"\n",
        "for index, paragraph_summarization in enumerate(paragraph_summarizations):\n",
        "    collected_summarization += f\"Summary of segment {index + 1}: {paragraph_summarization}\\n\"\n",
        "\n",
        "print(collected_summarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y08WjQxiuZ0k"
      },
      "source": [
        "#####Step2: After obtaining summaries for each smaller text piece separately, process these summaries to generate the final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4GEhPEIudBe"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Multi-Stage Summarization: Total { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"在 500 字以內寫出以下文字的簡潔摘要：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdTjmyvfZwar"
      },
      "source": [
        "The code block below takes about **20** seconds to run when using the (1) **gemini-pro** model, (2) length of chunks is 512 and (3) maximum number of tokens is 550, but the actual time may vary depending on the condition of Colab and the status of the Google API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x5fn6I-msGC"
      },
      "outputs": [],
      "source": [
        "# Finally, we compile a final summary from the summaries of each section.\n",
        "\n",
        "# Record the start time.\n",
        "start = time.time()\n",
        "\n",
        "# Run final summarization.\n",
        "summarization_prompt = summarization_prompt_template.replace(\"<text>\", collected_summarization)\n",
        "final_summarization = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "# Calculate the execution time and round it to 2 decimal places.\n",
        "cost_time = round(time.time() - start, 2)\n",
        "\n",
        "# Print the summary and its length.\n",
        "print(f\"----------------------------Final Summary----------------------------\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 80):\n",
        "        print(f\"{text}\")\n",
        "print(f\"\\nLength of final summary: {len(final_summarization)}\")\n",
        "print(f\"Time taken to generate the final summary: {cost_time} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPR2Am6omt0U"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-gemini-multi-stage.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    final_summarization = cc.convert(final_summarization)\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(final_summarization)\n",
        "\n",
        "print(f\"Final summary has been saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDZVeHCKvcMy"
      },
      "source": [
        "#### **If you want to use the method of Refinement, begin with this part.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1goEansHvc8C"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Refinement { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template** and **summarization_prompt_refine_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "\n",
        "# Initial prompt.\n",
        "# @markdown **summarization_prompt_template**: Initial prompt.\n",
        "summarization_prompt_template = \"請在 300 字以內，提供以下文字的簡潔摘要:<text>\" # @param {type:\"string\"}\n",
        "\n",
        "# Refinement prompt.\n",
        "# @markdown **summarization_prompt_refinement_template**: Refinement prompt.\n",
        "summarization_prompt_refinement_template = \"請在 500 字以內，結合原先的摘要和新的內容，提供簡潔的摘要:<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5AunhcUwDWh"
      },
      "source": [
        "Pipeline of the method of Refinement.\n",
        "\n",
        "Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "\n",
        "Step2: For each following document, the previous output is fed in along with the new document.\n",
        "\n",
        "Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "\n",
        "Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJMgqTC_2WD0"
      },
      "source": [
        "The code block below takes about **45** seconds to run when using the (1) **gemini-pro** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the Google API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDWW_K4OwG1N"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    if index == 0:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Construct summarization prompt.\n",
        "        summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "        # Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "        first_paragraph_summarization = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(first_paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # Print the summary and its length.\n",
        "        print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarizations, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for segment {index + 1}: {len(paragraph_summarizations)}\")\n",
        "        print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    else:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Step2: For each following document, the previous output is fed in along with the new document.\n",
        "        chunk_text = f\"\"\"前 {index} 段的摘要: {paragraph_summarizations[-1]}\\n第 {index + 1} 段的內容: {chunk}\"\"\"\n",
        "\n",
        "        # Construct refinement prompt for summarization.\n",
        "        summarization_prompt = summarization_prompt_refinement_template.replace(\"<text>\", chunk_text)\n",
        "\n",
        "        # Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "        paragraph_summarization = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # print results.\n",
        "        print(f\"----------------------------Summary of the First {index + 1} Segments----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for the first {index + 1} segments: {len(paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for the first {index + 1} segments: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5jw0HwWwW83"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-gemini-refinement.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    paragraph_summarizations[-1] = cc.convert(paragraph_summarizations[-1])\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(paragraph_summarizations[-1])\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(paragraph_summarizations[-1])} words) =====\\n\")\n",
        "for text in textwrap.wrap(paragraph_summarizations[-1], 64):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3rwvZYdXieB"
      },
      "source": [
        "## **If you want to use Claude, begin with this part.**\n",
        "##### (1) You can refer to https://reurl.cc/yLy06D for obtaining Claude API key.\n",
        "##### (2) You can refer to https://docs.anthropic.com/claude/docs/models-overview for more details about which models you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-51sj3lXieC"
      },
      "outputs": [],
      "source": [
        "def summarization(client, summarization_prompt, model_name=\"claude-3-sonnet-20240229\", temperature=0.0, top_p=1.0, max_tokens=512):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - Use the Claude API to summarize a given text.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - client: Claude API client.\n",
        "        - text: The text to be summarized.\n",
        "        - summarization_prompt: The summarization prompt.\n",
        "        - model_name: The model name, default is \"claude-3-sonnet-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "        - temperature: Controls randomness in the response. Lower values make responses more deterministic, default is 0.0.\n",
        "        - top_p: Controls diversity via nucleus sampling. Higher values lead to more diverse responses, default is 1.0.\n",
        "        - max_tokens: The maximum number of tokens to generate in the completion, default is 512.\n",
        "\n",
        "    (3) Return:\n",
        "        - The summarized text.\n",
        "\n",
        "    (4) Example:\n",
        "        - If the text is \"ABC\" and the summarization prompt is \"DEF\", system_prompt is \"GHI\", model_name is \"claude-3-sonnet-20240229\",\n",
        "          temperature is 0.0, top_p is 1.0, and max_tokens is 512, then you can call the function like this:\n",
        "\n",
        "              summarization(client=client, text=\"ABC\", summarization_prompt=\"DEF\", system_prompt=\"GHI\", model_name=\"claude-3-sonnet-20240229\", temperature=0.0, top_p=1.0, max_tokens=512)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = summarization_prompt\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            # Use the Claude API to summarize the text.\n",
        "            message = client.messages.create(\n",
        "                model=model_name,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            break\n",
        "\n",
        "        except:\n",
        "            # If the API call fails, wait for 1 second and try again.\n",
        "            print(\"The API call fails, wait for 1 second and try again.\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return message.content[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mfcYh1Rwkrh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of Claude { run: \"auto\" }\n",
        "''' ===== In this block, you can modify your desired parameters and set your Claude API key ===== '''\n",
        "\n",
        "# Your Claude API key.\n",
        "# @markdown **claude_api_key**: Your Claude api key.\n",
        "claude_api_key = \"YOUR_CLAUDE_API_KEY\" # @param {type:\"string\"}\n",
        "\n",
        "# The model name, default is \"claude-3-opus-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "# @markdown **model_name**: The model name, default is \"claude-3-opus-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "model_name = \"claude-3-opus-20240229\" # @param {type:\"string\"}\n",
        "\n",
        "# Controls randomness in the response. Lower values make responses more deterministic.\n",
        "# @markdown **temperature**: Controls randomness in the response. Lower values make responses more deterministic.\n",
        "temperature = 1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "# @markdown **top_p**: Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "top_p = 1.0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-ivbHl6wrIj"
      },
      "outputs": [],
      "source": [
        "# Construct Claude client.\n",
        "client = anthropic.Anthropic(api_key=claude_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eDZJpX7xagQ"
      },
      "source": [
        "### We offer the following two methods for summarization.\n",
        "Reference: https://reurl.cc/VzagLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEmYimdXxctB"
      },
      "source": [
        "#### **If you want to use the method of Multi-Stage Summarization, begin with this part.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4OBcswSxfUp"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Claude Multi-Stage Summarization: Paragraph { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# The maximum number of tokens to generate in the completion.\n",
        "# @markdown **max_tokens**: The maximum number of tokens to generate in the completion.\n",
        "max_tokens = 350 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"用 300 個字內寫出這段文字的摘要，其中包括要點和所有重要細節：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WClFqWBsyUWB"
      },
      "source": [
        "##### Step1: Split the long text into multiple smaller pieces and obtain summaries for each smaller text piece separately"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avQs_YFg2mK7"
      },
      "source": [
        "The code block below takes about **120** seconds to run when using the (1) **claude-3-opus-20240229** model, (2) length of chunks is 512 and (3) maximum number of tokens is 350, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJlEpq4EqIAN"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    # Record the start time.\n",
        "    start = time.time()\n",
        "\n",
        "    # Construct summarization prompt.\n",
        "    summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "    # We summarize each section that has been split up separately.\n",
        "    response = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "    # Calculate the execution time and round it to 2 decimal places.\n",
        "    cost_time = round(time.time() - start, 2)\n",
        "\n",
        "    # Print the summary and its length.\n",
        "    print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "    for text in textwrap.wrap(response, 80):\n",
        "        print(f\"{text}\\n\")\n",
        "    print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "    print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Record the result.\n",
        "    paragraph_summarizations.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vzgd8rWqJP9"
      },
      "outputs": [],
      "source": [
        "# First, we collect all the summarizations obtained before and print them.\n",
        "\n",
        "collected_summarization = \"\"\n",
        "for index, paragraph_summarization in enumerate(paragraph_summarizations):\n",
        "    collected_summarization += f\"Summary of segment {index + 1}: {paragraph_summarization}\\n\\n\"\n",
        "\n",
        "print(collected_summarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qDxoyLpymA5"
      },
      "source": [
        "##### Step2: After obtaining summaries for each smaller text piece separately, process these summaries to generate the final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E99bQfbgynXT"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Multi-Stage Summarization: Total { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"在 500 字以內寫出以下文字的簡潔摘要：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-WvuFl4XV2J"
      },
      "source": [
        "The code block below takes about **25** seconds to run when using the (1) **claude-3-opus-20240229** model, (2) length of chunks is 512 and (3) maximum number of tokens is 550, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qX4pa5QqKrc"
      },
      "outputs": [],
      "source": [
        "# Finally, we compile a final summary from the summaries of each section.\n",
        "\n",
        "# Record the start time.\n",
        "start = time.time()\n",
        "\n",
        "summarization_prompt = summarization_prompt_template.replace(\"<text>\", collected_summarization)\n",
        "\n",
        "# Run final summarization.\n",
        "final_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "# Calculate the execution time and round it to 2 decimal places.\n",
        "cost_time = round(time.time() - start, 2)\n",
        "\n",
        "# Print the summary and its length.\n",
        "print(f\"----------------------------Final Summary----------------------------\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 80):\n",
        "    print(f\"{text}\")\n",
        "print(f\"\\nLength of final summary: {len(final_summarization)}\")\n",
        "print(f\"Time taken to generate the final summary: {cost_time} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldTq9WYlqL2U"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-claude-multi-stage.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    final_summarization = cc.convert(final_summarization)\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(final_summarization)\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(final_summarization)} words) =====\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 64):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBI-Ba8QzLU8"
      },
      "source": [
        "#### **If you want to use the method of Refinement, begin with this part.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t2muOwUzNd0"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Claude Refinement { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template** and **summarization_prompt_refine_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "\n",
        "# Initial prompt.\n",
        "# @markdown **summarization_prompt_template**: Initial prompt.\n",
        "summarization_prompt_template = \"請在 300 字以內，提供以下文字的簡潔摘要:<text>\" # @param {type:\"string\"}\n",
        "\n",
        "# Refinement prompt.\n",
        "# @markdown **summarization_prompt_refinement_template**: Refinement prompt.\n",
        "summarization_prompt_refinement_template = \"請在 500 字以內，結合原先的摘要和新的內容，提供簡潔的摘要:<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JxW1AR2zZDL"
      },
      "source": [
        "Pipeline of the method of Refinement.\n",
        "\n",
        "Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "\n",
        "Step2: For each following document, the previous output is fed in along with the new document.\n",
        "\n",
        "Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "\n",
        "Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkm60apd2tQd"
      },
      "source": [
        "The code block below takes about **150** seconds to run when using the (1) **claude-3-opus-20240229** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Feco4LK4zazs"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    if index == 0:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Construct summarization prompt.\n",
        "        summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "        # Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "        first_paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(first_paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # Print the summary and its length.\n",
        "        print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "        for text in textwrap.wrap(response, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "        print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Step2: For each following document, the previous output is fed in along with the new document.\n",
        "        chunk_text = f\"\"\"前 {index} 段的摘要: {paragraph_summarizations[-1]}\\n第 {index + 1} 段的內容: {chunk}\"\"\"\n",
        "\n",
        "        # Construct refinement prompt for summarization.\n",
        "        summarization_prompt = summarization_prompt_refinement_template.replace(\"<text>\", chunk_text)\n",
        "\n",
        "        # Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "        paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # print results.\n",
        "        print(f\"----------------------------Summary of the First {index + 1} Segments----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for the first {index + 1} segments: {len(paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for the first {index + 1} segments: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4BGRgvFz0eW"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-claude-refinement.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    paragraph_summarizations[-1] = cc.convert(paragraph_summarizations[-1])\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(paragraph_summarizations[-1])\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(paragraph_summarizations[-1])} words) =====\\n\")\n",
        "for text in textwrap.wrap(paragraph_summarizations[-1], 80):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtQoyYjfdQLC"
      },
      "source": [
        "# Part5 - Check the correctness of the submission file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gclg3cORdiVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ddbc571-52ec-4f81-a114-145507c6a2cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The format of your submission file is correct.\n",
            "Your final score is 100.\n"
          ]
        }
      ],
      "source": [
        "# Check the correctness of the submission file.\n",
        "import json\n",
        "import re\n",
        "\n",
        "your_submission_path = \"YOUR_SUBMISSION_PATH\"\n",
        "\n",
        "def check_format(your_submission_path):\n",
        "\n",
        "    final_score = 0\n",
        "\n",
        "    # check the extension of the file.\n",
        "    if not your_submission_path.endswith(\".json\"):\n",
        "        print(\"Please save your submission file in JSON format.\")\n",
        "\n",
        "    else:\n",
        "        try:\n",
        "            with open(your_submission_path, \"r\") as fp:\n",
        "                your_submission = json.load(fp)\n",
        "\n",
        "            evaluation_result = your_submission[\"history\"][0][\"messages\"][1][\"content\"]\n",
        "\n",
        "            if \"總分：\" not in evaluation_result:\n",
        "                # Correct format: 總分: <你的分數>\n",
        "                print(\"Please make sure that the correct format of final score is included in the evaluation result.\")\n",
        "                print(\"The correct format is 總分: <你的分數>. For example, 總分: 97\")\n",
        "                return False, final_score\n",
        "\n",
        "            evaluation_result = evaluation_result.strip()\n",
        "            score_pattern = r\"總分：\\d+\"\n",
        "            score = re.findall(score_pattern, evaluation_result)\n",
        "\n",
        "            if score:\n",
        "                final_score = score[-1].replace(\"總分：\", \"\")\n",
        "                if \"/100\" in final_score:\n",
        "                    final_score = final_score.replace(\"/100\", \"\")\n",
        "            else:\n",
        "                print(\"Please make sure that the final score is included in the evaluation result.\")\n",
        "                return False, final_score\n",
        "\n",
        "        except:\n",
        "            print(\"Open the file failed. Please check the file path or save your submission file in correct JSON format\")\n",
        "            return False, final_score\n",
        "\n",
        "    return True, final_score\n",
        "\n",
        "format_correctness, final_score = check_format(your_submission_path)\n",
        "if format_correctness== True:\n",
        "    print(\"The format of your submission file is correct.\")\n",
        "    print(f\"Your final score is {final_score}.\")\n",
        "else:\n",
        "    print(\"The format of your submission file is wrong.\")\n",
        "    print(\"Please check the format of your submission file.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "voEioD2DCoeq",
        "z9QC8lG_QRZL",
        "hRzf_0cTV6TS",
        "GEmYimdXxctB",
        "eBI-Ba8QzLU8"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c16f77332c4442f900d74757e09a4c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4436a5a5d544412187dd9e0bcc381dc3",
              "IPY_MODEL_63014e48ddab48ae8364a5be1a5b35d4",
              "IPY_MODEL_1da20e27b5204b1d8f73d2997bca7da8"
            ],
            "layout": "IPY_MODEL_3d151547c8184fd79942d23ff6f07433"
          }
        },
        "4436a5a5d544412187dd9e0bcc381dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a303666c673547b3bf806cbe69c23004",
            "placeholder": "​",
            "style": "IPY_MODEL_d3ddc5c50e9f403eabaaa81f4a455228",
            "value": "Downloading readme: 100%"
          }
        },
        "63014e48ddab48ae8364a5be1a5b35d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27f2d2759ba04a82b6869a4ab53b3d5a",
            "max": 305,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03c027ccf19042988e6e6bf5c338a331",
            "value": 305
          }
        },
        "1da20e27b5204b1d8f73d2997bca7da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76d3e57c1cd54ab0bfe98ff2a8087b79",
            "placeholder": "​",
            "style": "IPY_MODEL_959e8d6436af400fa49efcc16e699c32",
            "value": " 305/305 [00:00&lt;00:00, 21.3kB/s]"
          }
        },
        "3d151547c8184fd79942d23ff6f07433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a303666c673547b3bf806cbe69c23004": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3ddc5c50e9f403eabaaa81f4a455228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27f2d2759ba04a82b6869a4ab53b3d5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03c027ccf19042988e6e6bf5c338a331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76d3e57c1cd54ab0bfe98ff2a8087b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "959e8d6436af400fa49efcc16e699c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a1c378508a74989a07cb7093e468541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f507507f226c4061bf48a464305937a6",
              "IPY_MODEL_36072d0d3103470e95ca68332df2d9ac",
              "IPY_MODEL_1109f950409848638fb337ec956c7ddc"
            ],
            "layout": "IPY_MODEL_5dc61916d95748e0b947fedc08d9e5a3"
          }
        },
        "f507507f226c4061bf48a464305937a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38b1ce0d475046f4b11c596ceee21d12",
            "placeholder": "​",
            "style": "IPY_MODEL_6fb8fbb7775940fcae0b3fef018fdfab",
            "value": "Downloading data: 100%"
          }
        },
        "36072d0d3103470e95ca68332df2d9ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcaa00c15ade43518df22682e5c1285f",
            "max": 3140168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b241cd8bfccf4f059fc6cba78385195f",
            "value": 3140168
          }
        },
        "1109f950409848638fb337ec956c7ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9acf964f8884dbeb76ee96d224abe4e",
            "placeholder": "​",
            "style": "IPY_MODEL_1b0dbe0d59d5477c8ea376aa5eb15b99",
            "value": " 3.14M/3.14M [00:00&lt;00:00, 8.62MB/s]"
          }
        },
        "5dc61916d95748e0b947fedc08d9e5a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38b1ce0d475046f4b11c596ceee21d12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fb8fbb7775940fcae0b3fef018fdfab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcaa00c15ade43518df22682e5c1285f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b241cd8bfccf4f059fc6cba78385195f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9acf964f8884dbeb76ee96d224abe4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0dbe0d59d5477c8ea376aa5eb15b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bca4eaad9f84858ad34cda4c434d9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58518e26240b42649b9fcb26e1d78cd1",
              "IPY_MODEL_b4e613d14fca4d4395e3b3830213cf4b",
              "IPY_MODEL_370772192f70424a9e699e26044c8d64"
            ],
            "layout": "IPY_MODEL_8c7aad1c38844ac4b132f92c5b67a0c0"
          }
        },
        "58518e26240b42649b9fcb26e1d78cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3410dce4c170459884c3677b75a7d8ff",
            "placeholder": "​",
            "style": "IPY_MODEL_0f17d82d06bb47caa9b1c9094cac405f",
            "value": "Generating test split: 100%"
          }
        },
        "b4e613d14fca4d4395e3b3830213cf4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4718e80a37c4a62b99ef3d28f99f733",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f30b4639aba4a098316880d7b21e276",
            "value": 1
          }
        },
        "370772192f70424a9e699e26044c8d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f44598d1ddf445eb12815f2cdc440fa",
            "placeholder": "​",
            "style": "IPY_MODEL_7618d752971440e980090269ac512bbb",
            "value": " 1/1 [00:00&lt;00:00, 16.36 examples/s]"
          }
        },
        "8c7aad1c38844ac4b132f92c5b67a0c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3410dce4c170459884c3677b75a7d8ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f17d82d06bb47caa9b1c9094cac405f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4718e80a37c4a62b99ef3d28f99f733": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f30b4639aba4a098316880d7b21e276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f44598d1ddf445eb12815f2cdc440fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7618d752971440e980090269ac512bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}